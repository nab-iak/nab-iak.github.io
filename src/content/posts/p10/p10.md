---
title: '使用mlr3进行机器学习'
published: 2025-06-14
image: './p10_cover.jpg'
description: '内容太多了打开会卡，以后分章'
category: 'Machine Learning'
tags: [mlr3,R,ZH-CN]
draft: false 
lang: 'en'
---
> - Cover Pic by [@抠肉肚脐](https://www.pixiv.net/artworks/126561187)  

> [Applied Machine Learning Using mlr3 in R](https://mlr3book.mlr-org.com/)

## 机器学习流程

![image_1](./image_1.png)

1. 将数据集拆分为训练集与测试集
2. 选择学习器，用训练集构建模型
3. 使用构建的模型对测试集进行预测
4. 基于预测的结果对模型用不同的评估器进行评估
5. 基于评估结果评价模型的性能

## 预设字典

### 预设的Task列表

```r
mlr_tasks
```

```css
<DictionaryTask> with 11 stored values
Keys: breast_cancer, california_housing, german_credit, iris, mtcars,
  penguins, pima, sonar, spam, wine, zoo
```

### 预设学习器列表

```r
mlr_learners
```

```css
<DictionaryLearner> with 27 stored values
Keys: classif.cv_glmnet, classif.debug, classif.featureless,
  classif.glmnet, classif.kknn, classif.lda, classif.log_reg,
  classif.multinom, classif.naive_bayes, classif.nnet, classif.qda,
  classif.ranger, classif.rpart, classif.svm, classif.xgboost,
  regr.cv_glmnet, regr.debug, regr.featureless, regr.glmnet, regr.kknn,
  regr.km, regr.lm, regr.nnet, regr.ranger, regr.rpart, regr.svm,
  regr.xgboost
```

### 预设的评估器列表

```r
# 1
mlr_measures
# 2
msr()
```

```css
<DictionaryMeasure> with 62 stored values
Keys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,
  classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,
  classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,
  classif.logloss, classif.mauc_au1p, classif.mauc_au1u,
  classif.mauc_aunp, classif.mauc_aunu, classif.mauc_mu,
  classif.mbrier, classif.mcc, classif.npv, classif.ppv, classif.prauc,
  classif.precision, classif.recall, classif.sensitivity,
  classif.specificity, classif.tn, classif.tnr, classif.tp,
  classif.tpr, debug_classif, internal_valid_score, oob_error,
  regr.bias, regr.ktau, regr.mae, regr.mape, regr.maxae, regr.medae,
  regr.medse, regr.mse, regr.msle, regr.pbias, regr.pinball, regr.rmse,
  regr.rmsle, regr.rsq, regr.sae, regr.smape, regr.srho, regr.sse,
  selected_features, sim.jaccard, sim.phi, time_both, time_predict,
  time_train
```

```r
msr() %>% as.data.table()
```

<table class='dataframe'>
<caption>A data.table: 62 x 7</caption>
<thead>
 <tr><th scope=col>key</th><th scope=col>label</th><th scope=col>task_type</th><th scope=col>packages</th><th scope=col>predict_type</th><th scope=col>properties</th><th scope=col>task_properties</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>aic                </td><td>Akaike Information Criterion              </td><td>NA     </td><td>mlr3</td><td>NA      </td><td>na_score              , requires_learner      , requires_model        , requires_no_prediction</td><td></td></tr>
 <tr><td>bic                </td><td>Bayesian Information Criterion            </td><td>NA     </td><td>mlr3</td><td>NA      </td><td>na_score              , requires_learner      , requires_model        , requires_no_prediction</td><td></td></tr>
 <tr><td>classif.acc        </td><td>Classification Accuracy                   </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td>weights</td><td></td></tr>
 <tr><td>classif.auc        </td><td>Area Under the ROC Curve                  </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td>twoclass</td></tr>
 <tr><td>classif.bacc       </td><td>Balanced Accuracy                         </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td>weights</td><td></td></tr>
 <tr><td>classif.bbrier     </td><td>Binary Brier Score                        </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td>weights</td><td>twoclass</td></tr>
 <tr><td>classif.ce         </td><td>Classification Error                      </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td>weights</td><td></td></tr>
 <tr><td>classif.costs      </td><td>Cost-sensitive Classification             </td><td>classif</td><td>mlr3</td><td>response</td><td>weights</td><td></td></tr>
 <tr><td>classif.dor        </td><td>Diagnostic Odds Ratio                     </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.fbeta      </td><td>F-beta score                              </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.fdr        </td><td>False Discovery Rate                      </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.fn         </td><td>False Negatives                           </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.fnr        </td><td>False Negative Rate                       </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.fomr       </td><td>False Omission Rate                       </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.fp         </td><td>False Positives                           </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.fpr        </td><td>False Positive Rate                       </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.logloss    </td><td>Log Loss                                  </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td>weights</td><td></td></tr>
 <tr><td>classif.mauc_au1p  </td><td>Weighted average 1 vs. 1 multiclass AUC   </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td></td></tr>
 <tr><td>classif.mauc_au1u  </td><td>Average 1 vs. 1 multiclass AUC            </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td></td></tr>
 <tr><td>classif.mauc_aunp  </td><td>Weighted average 1 vs. rest multiclass AUC</td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td></td></tr>
 <tr><td>classif.mauc_aunu  </td><td>Average 1 vs. rest multiclass AUC         </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td></td></tr>
 <tr><td>classif.mauc_mu    </td><td>Multiclass mu AUC                         </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td></td></tr>
 <tr><td>classif.mbrier     </td><td>Multiclass Brier Score                    </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td></td></tr>
 <tr><td>classif.mcc        </td><td>Matthews Correlation Coefficient          </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td></td></tr>
 <tr><td>classif.npv        </td><td>Negative Predictive Value                 </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.ppv        </td><td>Positive Predictive Value                 </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.prauc      </td><td>Precision-Recall Curve                    </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob    </td><td></td><td>twoclass</td></tr>
 <tr><td>classif.precision  </td><td>Precision                                 </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.recall     </td><td>Recall                                    </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>classif.sensitivity</td><td>Sensitivity                               </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response</td><td></td><td>twoclass</td></tr>
 <tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>
 <tr><td>classif.tnr         </td><td>True Negative Rate                                 </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td>twoclass</td></tr>
 <tr><td>classif.tp          </td><td>True Positives                                     </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td>twoclass</td></tr>
 <tr><td>classif.tpr         </td><td>True Positive Rate                                 </td><td>classif</td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td>twoclass</td></tr>
 <tr><td>debug_classif       </td><td>Debug Classification Measure                       </td><td>NA     </td><td>mlr3</td><td>response </td><td>na_score</td><td></td></tr>
 <tr><td>internal_valid_score</td><td>Internal Validation Score                          </td><td>NA     </td><td>mlr3</td><td>NA       </td><td>na_score              , requires_learner      , requires_no_prediction</td><td></td></tr>
 <tr><td>oob_error           </td><td>Out-of-bag Error                                   </td><td>NA     </td><td>mlr3</td><td>NA       </td><td>na_score              , requires_learner      , requires_no_prediction</td><td></td></tr>
 <tr><td>regr.bias           </td><td>Bias                                               </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.ktau           </td><td>Kendall's tau                                      </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>regr.mae            </td><td>Mean Absolute Error                                </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.mape           </td><td>Mean Absolute Percent Error                        </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.maxae          </td><td>Max Absolute Error                                 </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>regr.medae          </td><td>Median Absolute Error                              </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>regr.medse          </td><td>Median Squared Error                               </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>regr.mse            </td><td>Mean Squared Error                                 </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.msle           </td><td>Mean Squared Log Error                             </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.pbias          </td><td>Percent Bias                                       </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.pinball        </td><td>NA                                                 </td><td>regr   </td><td>mlr3</td><td>quantiles</td><td></td><td></td></tr>
 <tr><td>regr.rmse           </td><td>Root Mean Squared Error                            </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.rmsle          </td><td>Root Mean Squared Log Error                        </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.rsq            </td><td>NA                                                 </td><td>regr   </td><td>mlr3</td><td>response </td><td>weights</td><td></td></tr>
 <tr><td>regr.sae            </td><td>Sum of Absolute Errors                             </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>regr.smape          </td><td>Symmetric Mean Absolute Percent Error              </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>regr.srho           </td><td>Spearman's rho                                     </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>regr.sse            </td><td>Sum of Squared Errors                              </td><td>regr   </td><td>mlr3        , mlr3measures</td><td>response </td><td></td><td></td></tr>
 <tr><td>selected_features   </td><td>Absolute or Relative Frequency of Selected Features</td><td>NA     </td><td>mlr3</td><td>NA       </td><td>requires_task         , requires_learner      , requires_model        , requires_no_prediction</td><td></td></tr>
 <tr><td>sim.jaccard         </td><td>Jaccard Similarity Index                           </td><td>NA     </td><td>mlr3        , mlr3measures</td><td>NA       </td><td>requires_model        , requires_no_prediction</td><td></td></tr>
 <tr><td>sim.phi             </td><td>Phi Coefficient Similarity                         </td><td>NA     </td><td>mlr3        , mlr3measures</td><td>NA       </td><td>requires_model        , requires_no_prediction</td><td></td></tr>
 <tr><td>time_both           </td><td>Elapsed Time                                       </td><td>NA     </td><td>mlr3</td><td>NA       </td><td>requires_learner      , requires_no_prediction</td><td></td></tr>
 <tr><td>time_predict        </td><td>Elapsed Time                                       </td><td>NA     </td><td>mlr3</td><td>NA       </td><td>requires_learner      , requires_no_prediction</td><td></td></tr>
 <tr><td>time_train          </td><td>Elapsed Time                                       </td><td>NA     </td><td>mlr3</td><td>NA       </td><td>requires_learner      , requires_no_prediction</td><td></td></tr>
</tbody>
</table>

### 调取详情

```r
mlr_tasks$get('california_housing')
mlr_learners$get('classif.xgboost')
mlr_measures$get('regr.mse')
```

```css
[36m--[39m [1m[34m<TaskRegr>[39m (20640x10): California House Value[22m [36m-------------------------------[39m
* Target: median_house_value
* Properties: -
* Features (9):
  * dbl (8): households, housing_median_age, latitude, longitude,
  median_income, population, total_bedrooms, total_rooms
  * fct (1): ocean_proximity



[36m--[39m [1m[34m<LearnerClassifXgboost>[39m (classif.xgboost): Extreme Gradient Boosting[22m [36m--------[39m
* Model: -
* Parameters: nrounds=1000, nthread=1, verbose=0
* Validate: [34m<NULL>[39m
* Packages: [34mmlr3[39m, [34mmlr3learners[39m, and [34mxgboost[39m
* Predict Types: [response] and prob
* Feature Types: logical, integer, and numeric
* Encapsulation: none (fallback: -)
* Properties: hotstart_forward, importance, internal_tuning, missings,
multiclass, offset, twoclass, validation, and weights
* Other settings: use_weights = 'use'



[36m--[39m [1m[34m<MeasureRegrSimple>[39m (regr.mse): Mean Squared Error[22m [36m--------------------------[39m
* Packages: [34mmlr3[39m and [34mmlr3measures[39m
* Range: [0, Inf]
* Minimize: [34mTRUE[39m
* Average: macro
* Parameters: list()
* Properties: weights
* Predict type: response
* Predict sets: test
* Aggregator: mean()
```

## 回归的流程

### 任务化

#### 创建任务

- `as_task_*()`
  - target：outcome
  - id：任务名称
- 这里就用`as_task_regr()`代替下

```r
dt.tskR <- as_task_regr(dt, target = 'outcome')
```

- 对outcome与特征值的图形化概括
  - 特征值过多不建议，又慢又看不清

```r
library(mlr3viz)
autoplot(dt.tskR, type = 'pairs')
```

#### 数据检索及数据处理

- 可以通过`$`探索任务里的各种信息
- 对于导入的原始数据而言
  - `dt.tskR$nrow`
  - `dt.tskR$ncol`
  - `dt.tskR$feature_names`
  - `dt.tskR$target_names`
  - `dt.tskR$row_ids`
  - `dt.tskR$col_info`
  - ......
- 有些是函数
  - `dt.tskR$clone()`
  - `dt.tskR$data()`
  - `dt.tskR$cbind()`
  - `dt.tskR$rbind()`
  - `dt.tskR$filter()`
  - `dt.tskR$select()`
  - `dt.tskR$head()`
  - ......

- 这两个虽然结果一样，  
  但是第一个是先调取整个data再筛选，  
  第二个则是直接充task里调取第二行

```r
housing.tskR$data()[2]
housing.tskR$data(rows = housing.tskR$row_ids[2])
```

- 函数的用法和平时的类似，只是集成到了Task里面而已

#### 数据处理的实例

- 注意，`dt.tskR$func()`是对`dt.tsk`的就地修改，  
  **不需要保存到新的变量里**

```r
# 克隆一个一模一样的任务
dt.tskR2 <- dt.tskR$clone()
# 选择第1–2行
dt.tskR2$filter(1:2)
```

```r
# 添加列
dt.tskR$cbind( 
  data.frame(x = c(150, 160))
)
# 添加行
dt.tskR_small$rbind( 
  data.frame(x1 = 23, x2 = 5, x3 = 170)
)
dt.tskR_small$data()
```

### 学习器
>
> [mlr-org: Learners](https://mlr-org.com/learners.html)

- 学习器的类型是`Learner`

#### 包的分类

- **mlr3**：仅包含一些基础的学习器
- **mlr3learners**：比`mlr3`包更丰富且常用的学习器
- **mlr3proba**：生存学习器
- **mlr3cluster**：聚类学习器
- **mlr3extralearners**：一些不稳定或者CRAN上不可用的学习器
- **mlr3torch**：神经网络相关学习器

#### 查看学习器

- `lrn()`可以调取对应学习器的信息，并且包含下列数据
  - `$feature_types`：学习器可以处理的特征类型
  - `$packages`：使用该学习器需要安装的软件包
  - `$properties`：学习者的属性；例如，**'missings'** 属性表示模型可以处理缺失数据，而 **'importance'** 表示它可以计算每个特征的相对重要性
  - `$predict_types`：模型能够做出的预测类型
  - `$param_set`：可用超参数集

```r
lrn('regr.rpart')
```

```css
[36m--[39m [1m[34m<LearnerRegrRpart>[39m (regr.rpart): Regression Tree[22m [36m----------------------------[39m
* Model: -
* Parameters: xval=0
* Packages: [34mmlr3[39m and [34mrpart[39m
* Predict Types: [response]
* Feature Types: logical, integer, numeric, factor, and ordered
* Encapsulation: none (fallback: -)
* Properties: importance, missings, selected_features, and weights
* Other settings: use_weights = 'use'
```

#### 传递多个学习器

- `lrn()`的复数形`lrns()`可以调取多个学习器
- 其他的类似的还有`tsks()`和后续出现的`msrs()`

```r
dt.lrnR.rpart_lm <- lrns(c('regr.rpart', 'regr.lm'))
dt.lrnR.rpart_lm
```

### 分割训练集和测试集

- `partition()`中`ratio`为训练集占整个数据集比例

```r
dt.tskR.splits <- partition(dt.tsk, ratio = 0.7)
dt.tskR.splits
```

### 训练模型

- `Learner`作用于`Task`

```r
dt.lrnR.rpart <- lrn('regr.rpart')
dt.lrnR.rpart$train(dt.tsk)
```

- 拟合的模型储存在`Learner$model`中

```r
dt.lrnR.rpart$model
```

- 通过指定参数`row_ids`分配训练集

```r
dt.lrnR.rpart$train(dt.tskR, row_ids = dt.tskR.splits$train)
```

- 也可以指定预测类型
  - `regr.rpart`不能选择response以外的类型
  - 这里选择用 `regr.lm`来预测标准误差

```r
dt.lrnR.lm <- lrn('regr.lm', predict_type = 'se')
```

- 可以随便看看区别
  - 在指定了**se**后output里`Predict Types`的框框给到了`[se]`

```r
lrn('regr.lm')
lrn('regr.lm', predict_type = 'se')
```

```css

[36m--[39m [1m[34m<LearnerRegrLM>[39m (regr.lm): Linear Model[22m [36m-------------------------------------[39m
* Model: -
* Parameters: use_pred_offset=TRUE
* Packages: [34mmlr3[39m, [34mmlr3learners[39m, and [34mstats[39m
* Predict Types: [response] and se
* Feature Types: logical, integer, numeric, character, and factor
* Encapsulation: none (fallback: -)
* Properties: offset and weights
* Other settings: use_weights = 'use'


[36m--[39m [1m[34m<LearnerRegrLM>[39m (regr.lm): Linear Model[22m [36m-------------------------------------[39m
* Model: -
* Parameters: use_pred_offset=TRUE
* Packages: [34mmlr3[39m, [34mmlr3learners[39m, and [34mstats[39m
* Predict Types: response and [se]
* Feature Types: logical, integer, numeric, character, and factor
* Encapsulation: none (fallback: -)
* Properties: offset and weights
* Other settings: use_weights = 'use'
```

### 预测数据

- 由于`Learner$predict()`会返回一个新的对象，所以需要另外保存

```r
dt.lrnR.rpart.pred <- dt.lrnR.rpart$predict(dt.tskR, row_ids = dt.tskR.splits$test)
```

- 可以简单看看这些对象的类别

```r
dt.tskR %>% class()
dt.lrnR.rpart %>% class()
dt.lrnR.rpart.pred %>% class()
```

```css
'TaskRegr''TaskSupervised''Task''R6'
'LearnerRegrRpart''LearnerRegr''Learner''R6'
'PredictionRegr''Prediction''R6'
```

- `dt.lrnR.rpart.pred`：返回一个预测结果的文本输出
- `dt.lrnR.rpart.pred$truth`：返回真实值向量
- `dt.lrnR.rpart.pred$responese`：返回预测响应值向量

### 对真实值-预测值进行绘图

```r
autoplot(dt.lrnR.rpart.pred)
```

### 超参数

- 训练模型前需要**预先指定的参数**

#### 获取超参数

```r
# 1
dt.lrnR.rpart$param_set
# 2
lrn('regr.rpart')$param_set
```

```css
<ParamSet(10)>
                id    class lower upper nlevels        default  value
            <char>   <char> <num> <num>   <num>         <list> <list>
 1:             cp ParamDbl     0     1     Inf           0.01 [NULL]
 2:     keep_model ParamLgl    NA    NA       2          FALSE [NULL]
 3:     maxcompete ParamInt     0   Inf     Inf              4 [NULL]
 4:       maxdepth ParamInt     1    30      30             30 [NULL]
 5:   maxsurrogate ParamInt     0   Inf     Inf              5 [NULL]
 6:      minbucket ParamInt     1   Inf     Inf <NoDefault[0]> [NULL]
 7:       minsplit ParamInt     1   Inf     Inf             20 [NULL]
 8: surrogatestyle ParamInt     0     1       2              0 [NULL]
 9:   usesurrogate ParamInt     0     2       3              2 [NULL]
10:           xval ParamInt     0   Inf     Inf             10      0
```

- 其中
  - **id**：超参名
  - **class**：类别
  - **lower**：最小可设定值
  - **upper**：最大可设定值
  - **nlevels**：可设定值的种类数目
  - **default**：原函数的默认取值
  - **value**：当前取值，`[NULL]`时使用`default`的值

- 当`cp`设置为`0.5`后，可以看到output中`cp`的value变成了`0.5`

```r
lrn('regr.rpart', cp = 0.5)$param_set
```

```css
<ParamSet(10)>
                id    class lower upper nlevels        default  value
            <char>   <char> <num> <num>   <num>         <list> <list>
 1:             cp ParamDbl     0     1     Inf           0.01    0.5
 2:     keep_model ParamLgl    NA    NA       2          FALSE [NULL]
 3:     maxcompete ParamInt     0   Inf     Inf              4 [NULL]
 4:       maxdepth ParamInt     1    30      30             30 [NULL]
 5:   maxsurrogate ParamInt     0   Inf     Inf              5 [NULL]
 6:      minbucket ParamInt     1   Inf     Inf <NoDefault[0]> [NULL]
 7:       minsplit ParamInt     1   Inf     Inf             20 [NULL]
 8: surrogatestyle ParamInt     0     1       2              0 [NULL]
 9:   usesurrogate ParamInt     0     2       3              2 [NULL]
10:           xval ParamInt     0   Inf     Inf             10      0
```

- 查看当前模型的超参设定情况

```r
dt.lrnR.rpart$param_set$values
```

- 对查看当前模型的超参设定就地修改

```r
dt.lrnR.rpart$param_set$set_values(cp = 0.02，minsplit = 10)
```

#### 超参依赖关系

- 支持向量机中的超参更加复杂

```r
lrn('regr.svm')$param_set
```

```css
<ParamSet(14)>
Key: <id>
           id    class lower upper nlevels        default parents  value
       <char>   <char> <num> <num>   <num>         <list>  <list> <list>
 1: cachesize ParamDbl  -Inf   Inf     Inf             40  [NULL] [NULL]
 2:     coef0 ParamDbl  -Inf   Inf     Inf              0  kernel [NULL]
 3:      cost ParamDbl     0   Inf     Inf              1    type [NULL]
 4:     cross ParamInt     0   Inf     Inf              0  [NULL] [NULL]
 5:    degree ParamInt     1   Inf     Inf              3  kernel [NULL]
 6:   epsilon ParamDbl     0   Inf     Inf            0.1    type [NULL]
 7:    fitted ParamLgl    NA    NA       2           TRUE  [NULL] [NULL]
 8:     gamma ParamDbl     0   Inf     Inf <NoDefault[0]>  kernel [NULL]
 9:    kernel ParamFct    NA    NA       4         radial  [NULL] [NULL]
10:        nu ParamDbl  -Inf   Inf     Inf            0.5    type [NULL]
11:     scale ParamUty    NA    NA     Inf           TRUE  [NULL] [NULL]
12: shrinking ParamLgl    NA    NA       2           TRUE  [NULL] [NULL]
13: tolerance ParamDbl     0   Inf     Inf          0.001  [NULL] [NULL]
14:      type ParamFct    NA    NA       2 eps-regression  [NULL] [NULL]
```

- 可以从`Learner$param_set$deps`中获得超参之间的依赖关系总结

```r
lrn('regr.svm')$param_set$deps
```

<table class='dataframe'>
<caption>A data.table: 6 x 3</caption>
<thead>
 <tr><th scope=col>id</th><th scope=col>on</th><th scope=col>cond</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>coef0  </td><td>kernel</td><td>polynomial    , sigmoid       , %s %%in%% {%s}</td></tr>
 <tr><td>cost   </td><td>type  </td><td>eps-regression, nu-regression , %s %%in%% {%s}</td></tr>
 <tr><td>degree </td><td>kernel</td><td>polynomial, %s == %s  </td></tr>
 <tr><td>epsilon</td><td>type  </td><td>eps-regression, %s == %s      </td></tr>
 <tr><td>gamma  </td><td>kernel</td><td>polynomial    , radial        , sigmoid       , %s %%in%% {%s}</td></tr>
 <tr><td>nu     </td><td>type  </td><td>nu-regression, %s == %s     </td></tr>
</tbody>
</table>

- 查看一下设置的条件

```r
for (i in 1:6){
 print(lrn('regr.svm')$param_set$deps[[i, 'cond']])
}
```

```css
CondAnyOf: x %in% {polynomial, sigmoid}
CondAnyOf: x %in% {eps-regression, nu-regression}
CondEqual: x == polynomial
CondEqual: x == eps-regression
CondAnyOf: x %in% {polynomial, radial, sigmoid}
CondEqual: x == nu-regression
```

- 输出中`x`是`on`的指代，  
  当被依赖的超参`on`的设置满足某条件时，  
  `id`中的超参才能被设置
- 如果`on`超参不满足条件，设置`id`超参会报错

#### 基准学习器

- `regr.featureless`
  - 它总是将新值预测为训练数据中目标的均值
  - 如果 `robust` 超参数设置为 `TRUE`，则为中位数
- 用于
  - 模型比较
  - 备用学习器

```r
lrn('regr.featureless')
```

### 模型评估

- `msr()`用于单个评估器的评估
- `msrs()`可以输入使用多个评估器名称的向量

```r
# 1
msrR.mse_mae <- msrs(c('regr.mse', 'regr.mae'))
housing.lrnR.rpart.pred$score(msrR.mse_mae)
# 2
housing.lrnR.rpart.pred$score(msrs(c('regr.mse', 'regr.mae')))
```

- `mlr3`还有一些其他非质量评价的评估指标
  - `msr('time_train')`：训练模型所花费的时间
  - `msr('time_predict')`：模型进行预测所需的时间
  - `msr('time_both')`：训练模型然后进行预测所花费的总时间
  - `msr('selected_features')`：  
   模型选择的特征数量，仅当模型具有`selected_features`属性时才可使用

- `Measure$properties`提供了评估器的属性

```r
msr('regr.mse')$properties
msr('time_train')$properties
```

```css
'weights'
'requires_learner''requires_no_prediction'
```

- 评估器的超参
  - 可以注意到`time_train`的`Parameters`是`list()`，  
   这代表是空的
  - `selected_features`的`Parameters`有`normalize=FALSE`，  
   说明可以提前设置

```r
msr('time_train')
msr('selected_features')
```

```css
[36m--[39m [1m[34m<MeasureElapsedTime>[39m (time_train): Elapsed Time[22m [36m-----------------------------[39m
* Packages: [34mmlr3[39m
* Range: [0, Inf]
* Minimize: [34mTRUE[39m
* Average: macro
* Parameters: list()
* Properties: requires_learner and requires_no_prediction
* Predict type: NA
* Predict sets:
* Aggregator: mean()


[36m--[39m [1m[34m<MeasureSelectedFeatures>[39m (selected_features): Absolute or Relative Frequency[22m
* Packages: [34mmlr3[39m
* Range: [0, Inf]
* Minimize: [34mTRUE[39m
* Average: macro
* Parameters: normalize=FALSE
* Properties: requires_task, requires_learner, requires_model, and
requires_no_prediction
* Predict type: NA
* Predict sets:
* Aggregator: mean()
```

- 访问评估器的超参也是用`$param_set`

```r
msr.sf <- msr('selected_features')
msr.sf$param_set
```

```css
<ParamSet(1)>
          id    class lower upper nlevels        default  value
      <char>   <char> <num> <num>   <num>         <list> <list>
1: normalize ParamLgl    NA    NA       2 <NoDefault[0]>  FALSE
```

- 修改

```r
# 1
msr.sf$param_set$set_values(normalize = TRUE)
# 2
msr.sf$param_set$normalize <- TRUE
```

## 分类的流程

- 与回归的流程相同，  
  只是基础对象分别继承自 `TaskClassif`、`LearnerClassif` 和 `MeasureClassif`
- 分类有**二元分类**和**多元分类**两种

### 任务化

- 这里就直接调取预设的任务进行练习

```r
mlr_tasks %>% as.data.table()
```

<table class='dataframe'>
<caption>A data.table: 11 x 14</caption>
<thead>
 <tr><th scope=col>key</th><th scope=col>label</th><th scope=col>task_type</th><th scope=col>nrow</th><th scope=col>ncol</th><th scope=col>properties</th><th scope=col>lgl</th><th scope=col>int</th><th scope=col>dbl</th><th scope=col>chr</th><th scope=col>fct</th><th scope=col>ord</th><th scope=col>pxc</th><th scope=col>dte</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
 <tr><td>breast_cancer     </td><td>Wisconsin Breast Cancer</td><td>classif</td><td>  683</td><td>10</td><td>twoclass</td><td> 0</td><td>0</td><td> 0</td><td>0</td><td> 0</td><td>9</td><td>0</td><td>0</td></tr>
 <tr><td>california_housing</td><td>California House Value </td><td>regr   </td><td>20640</td><td>10</td><td></td><td> 0</td><td>0</td><td> 8</td><td>0</td><td> 1</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>german_credit     </td><td>German Credit          </td><td>classif</td><td> 1000</td><td>21</td><td>twoclass</td><td> 0</td><td>3</td><td> 0</td><td>0</td><td>14</td><td>3</td><td>0</td><td>0</td></tr>
 <tr><td>iris              </td><td>Iris Flowers           </td><td>classif</td><td>  150</td><td> 5</td><td>multiclass</td><td> 0</td><td>0</td><td> 4</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>mtcars            </td><td>Motor Trends           </td><td>regr   </td><td>   32</td><td>11</td><td></td><td> 0</td><td>0</td><td>10</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>penguins          </td><td>Palmer Penguins        </td><td>classif</td><td>  344</td><td> 8</td><td>multiclass</td><td> 0</td><td>3</td><td> 2</td><td>0</td><td> 2</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>pima              </td><td>Pima Indian Diabetes   </td><td>classif</td><td>  768</td><td> 9</td><td>twoclass</td><td> 0</td><td>0</td><td> 8</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>sonar             </td><td>Sonar: Mines vs. Rocks </td><td>classif</td><td>  208</td><td>61</td><td>twoclass</td><td> 0</td><td>0</td><td>60</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>spam              </td><td>HP Spam Detection      </td><td>classif</td><td> 4601</td><td>58</td><td>twoclass</td><td> 0</td><td>0</td><td>57</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>wine              </td><td>Wine Regions           </td><td>classif</td><td>  178</td><td>14</td><td>multiclass</td><td> 0</td><td>2</td><td>11</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>zoo               </td><td>Zoo Animals            </td><td>classif</td><td>  101</td><td>17</td><td>multiclass</td><td>15</td><td>1</td><td> 0</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td></tr>
</tbody>
</table>

- 分别用`german_credit`和`penguins`作为二元分类和多元分类的例子

```r
gc.tskC <- tsk('german_credit')
peng.tskC <- tsk('penguins')

gc.tskC
peng.tskC
```

```css
[36m--[39m [1m[34m<TaskClassif>[39m (1000x21): German Credit[22m [36m--------------------------------------[39m
* Target: credit_risk
* Target classes: good (positive class, 70%), bad (30%)
* Properties: twoclass
* Features (20):
  * fct (14): credit_history, employment_duration, foreign_worker, housing,
  job, other_debtors, other_installment_plans, people_liable,
  personal_status_sex, property, purpose, savings, status, telephone
  * int (3): age, amount, duration
  * ord (3): installment_rate, number_credits, present_residence


[36m--[39m [1m[34m<TaskClassif>[39m (344x8): Palmer Penguins[22m [36m--------------------------------------[39m
* Target: species
* Target classes: Adelie (44%), Gentoo (36%), Chinstrap (20%)
* Properties: multiclass
* Features (7):
  * int (3): body_mass, flipper_length, year
  * dbl (2): bill_depth, bill_length
  * fct (2): island, sex
```

### 二元分类

#### Outcome的分类

```r
gc.tskC$class_names
gc.tskC$properties
```

```css
'good''bad'
'twoclass'
```

#### 定义正类

```r
gc.tskC$positive <- 'good'
```

- 如果不设置，默认第一个级别是正类

#### 对真实值-预测值进行绘图

```r
autoplot(gc.tskC, type = 'duo') +
  theme(strip.text.y = element_text(angle = -45, size = 8))
```

#### 预测结果

- `predict_type`可以选择预测响应类别`'response'`或者概率`'prob'`
  - 选择概率也会给出`response`的结果
  - 所以对于分类问题，用概率会更好

```r
lrn('classif.rpart') # 'response'是默认
lrn('classif.rpart', predict_type = 'prob')
```

```r
# gc.tskC.splits <- partition(gc.tskC,ratio = 0.7) # 事先设定好

gc.lrnC.rpart <- lrn('classif.rpart', predict_type = 'response')
gc.lrnC.rpart$train(gc.tskC, gc.tskC.splits$train)
gc.lrnC.rpart.pred <- gc.lrnC.rpart$predict(gc.tskC, gc.tskC.splits$test)
gc.lrnC.rpart.pred
```

```css
[36m--[39m [1m[34m<PredictionClassif>[39m for [34m300[39m observations:[22m [36m-----------------------------------[39m
 row_ids truth response
       6  good     good
       8  good      bad
      10   bad      bad
     ---   ---      ---
     992  good     good
     995  good     good
    1000  good      bad
```

```r
# gc.tskC.splits <- partition(gc.tskC,ratio = 0.7) # 事先设定好

gc.lrnC.rpart <- lrn('classif.rpart', predict_type = 'prob')
gc.lrnC.rpart$train(gc.tskC, gc.tskC.splits$train)
gc.lrnC.rpart.pred <- gc.lrnC.rpart$predict(gc.tskC, gc.tskC.splits$test)
gc.lrnC.rpart.pred
```

```css
[36m--[39m [1m[34m<PredictionClassif>[39m for [34m300[39m observations:[22m [36m-----------------------------------[39m
 row_ids truth response  prob.good  prob.bad
       6  good     good 0.85667752 0.1433225
       8  good      bad 0.21739130 0.7826087
      10   bad      bad 0.29411765 0.7058824
     ---   ---      ---        ---       ---
     992  good     good 0.85667752 0.1433225
     995  good     good 0.85667752 0.1433225
    1000  good      bad 0.07692308 0.9230769
```

#### 模型评估

- 不同种类的评估器对于`response`和`prob`的适用性是不同的，  
  需要找到合适的评估器

- 寻找支持分类并用于评估概率的评估器

```r
as.data.table(msr())[
    task_type == 'classif' & predict_type == 'prob']
```

<table class='dataframe'>
<caption>A data.table: 7 x 7</caption>
<thead>
 <tr><th scope=col>key</th><th scope=col>label</th><th scope=col>task_type</th><th scope=col>packages</th><th scope=col>predict_type</th><th scope=col>properties</th><th scope=col>task_properties</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>classif.logloss  </td><td>Log Loss                                  </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob</td><td>weights</td><td></td></tr>
 <tr><td>classif.mauc_au1p</td><td>Weighted average 1 vs. 1 multiclass AUC   </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob</td><td></td><td></td></tr>
 <tr><td>classif.mauc_au1u</td><td>Average 1 vs. 1 multiclass AUC            </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob</td><td></td><td></td></tr>
 <tr><td>classif.mauc_aunp</td><td>Weighted average 1 vs. rest multiclass AUC</td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob</td><td></td><td></td></tr>
 <tr><td>classif.mauc_aunu</td><td>Average 1 vs. rest multiclass AUC         </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob</td><td></td><td></td></tr>
 <tr><td>classif.mauc_mu  </td><td>Multiclass mu AUC                         </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob</td><td></td><td></td></tr>
 <tr><td>classif.mbrier   </td><td>Multiclass Brier Score                    </td><td>classif</td><td>mlr3        , mlr3measures</td><td>prob</td><td></td><td></td></tr>
</tbody>
</table>

- 这里使用了3种评估器
  - `classif.mbrier`：用预测概率与真实值之间的平方差评估`prob`的预测结果
  - `classif.logloss`：用真实类别的预测概率的负对数评估`prob`的预测结果
  - `classif.acc`：评估`response`的预测分类结果

```r
gc.lrnC.rpart.pred$score(msrs(c('classif.mbrier', 'classif.logloss', 'classif.acc')))
```

#### 混淆矩阵

- `Prediction$confution`查看混淆矩阵

```r
gc.lrnC.rpart.pred$confusion
```

```css
        truth
response good bad
    good  177  45
    bad    41  37
```

- 通过`autoplot(Prediction)`可视化混淆矩阵

```r
autoplot(gc.lrnC.rpart.pred)
```

#### 阈值设定

- `Prediction$set_threshold()`设定阈值
  - 阈值是**判断为正类**的阈值，因此需要正确设置正类

```r
gc.lrnC.rpart.pred$set_threshold(YourThreshold)
```

### 多元分类

#### Outcome的分类

```r
peng.tskC$class_names
peng.tskC$properties
```

```css
'Adelie''Chinstrap''Gentoo'
'multiclass'
```

#### 没有定义正类

- 定义了反而会报错的

```r
peng.tskC$positive <- 'Adelie'
```

```css
Error in (function (rhs) : Setting the positive class is only feasible for binary classification
```

#### 对真实值-预测值进行绘图

```r
autoplot(peng.tskC, type = 'duo') +
  theme(strip.text.y = element_text(angle = -45, size = 8))
```

#### 预测结果

```r
# peng.tskC.splits <- partition(peng.tskC,ratio = 0.7) # 事先设定好

peng.lrnC.rpart <- lrn('classif.rpart', predict_type = 'response')
peng.lrnC.rpart$train(peng.tskC, peng.tskC.splits$train)
peng.lrnC.rpart.pred <- peng.lrnC.rpart$predict(peng.tskC, peng.lrnC.splits$test)
peng.lrnC.rpart.pred
```

```css
[36m--[39m [1m[34m<PredictionClassif>[39m for [34m103[39m observations:[22m [36m-----------------------------------[39m
 row_ids     truth  response
      11    Adelie    Adelie
      13    Adelie    Adelie
      14    Adelie    Adelie
     ---       ---       ---
     326 Chinstrap Chinstrap
     331 Chinstrap    Adelie
     338 Chinstrap Chinstrap
```

```r
# peng.tskC.splits <- partition(peng.tskC,ratio = 0.7) # 事先设定好

peng.lrnC.rpart <- lrn('classif.rpart', predict_type = 'prob')
peng.lrnC.rpart$train(peng.tskC, peng.tskC.splits$train)
peng.lrnC.rpart.pred <- peng.lrnC.rpart$predict(peng.tskC, peng.lrnC.splits$test)
peng.lrnC.rpart.pred
```

```css
[36m--[39m [1m[34m<PredictionClassif>[39m for [34m103[39m observations:[22m [36m-----------------------------------[39m
 row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
      11    Adelie    Adelie   0.9537037     0.01851852  0.02777778
      13    Adelie    Adelie   0.9537037     0.01851852  0.02777778
      14    Adelie    Adelie   0.9537037     0.01851852  0.02777778
     ---       ---       ---         ---            ---         ---
     326 Chinstrap Chinstrap   0.0000000     1.00000000  0.00000000
     331 Chinstrap    Adelie   0.9537037     0.01851852  0.02777778
     338 Chinstrap Chinstrap   0.0000000     1.00000000  0.00000000
```

#### 模型评估

- 寻找支持多元分类并用于评估概率的评估器

```r
as.data.table(msr())[
    task_type == 'classif' & predict_type == 'prob' &
    !sapply(task_properties, function(x) return('twoclass' %in% x))]
```

```r
peng.lrnC.rpart.pred$score(msrs(c('classif.mbrier', 'classif.logloss', 'classif.acc')))
```

#### 混淆矩阵

- `Prediction$confution`查看混淆矩阵

```r
gc.lrnC.rpart.pred$confusion
```

```css
        truth
response good bad
    good  177  45
    bad    41  37
```

- 通过`autoplot(Prediction)`可视化混淆矩阵

```r
autoplot(gc.lrnC.rpart.pred)
```

#### 阈值设定

- `Prediction$set_threshold()`设定阈值
  - 与二元分类不同的是，多元分类需要**给每一个分类**单独设置一个阈值

- 在多分类问题中阈值化的工作原理
  - 首先为 `n` 个类别中的每个类别分配一个阈值，  
   将每个类别的预测概率除以这些阈值，  
   以返回 `n` 个比率，然后选择比率最高的类别  
   （这里的阈值其实就是权重了，阈值越小，权重越高）

- 通过传递一个命名向量进行更改

```r
# 随便设置
peng.lrnC.rpart.pred$set_threshold(c(
 'Adelie' = 0.8,
 'Chinstrap' = 0.5,
 'Gentoo' = 0.2
))
```

- 逆加权设定阈值
  - 将新阈值设置为训练集中每个类别的比例
  - 在选择比率最高的标签之前，用预测概率除以这些类别比例

```r
# 逆加权法设置
peng.lrnC.rpart.pred$set_threshold(
 peng.tskC$truth(peng.tskC.splits$train) %>% 
  table() %>% 
  proportions()
)
```

## 定义数据role

- `Task$col_roles`能显示所有的role

```r
peng.tskC$col_roles
```

```css
$feature
'bill_depth''bill_length''flipper_length''island''sex''year''body_mass'
$target
'species'
$name
$order
$stratum
$group
$offset
$weights_learner
$weights_measure
```

- 通过`Task$set_col_roles('col', roles = 'role')`来重新定义

```r
set_col_roles('body_mass', roles = 'feature')
```

## 评估与基准测试

### 留出法与评分

- 留出法就是`partition()`的简单划分方法
- 评估`Prediction$score`是用来估计最终模型的**泛化能力**
![image_2](./image_2.png)

## 重采样

### 简介

![image_3](./image_3.png)
**k折交叉验证（k-fold cross-validation, CV）**

- 将数据集拆分为k折，其中k-1折用于模型训练，剩下1折用于测试数据
- 过程重复进行，直到每一折都被作为测试集使用一次
- 将k个模型的性估计值汇总，通常取平均值

- **留一法交叉验证（leave-one-out cross-validation, LOO-CV）**
  - CV的变体，用于测试集的数据不是一折而是**一个样本**
- **子采样（subsampling）**
  - 按一定比例**无放回**地抽取数据用作训练集，剩余的样本作为测试集
  - 与CV不同的是，每次抽取都是随机的，意味着有的样本会被重复抽取
  - 平均而言，在自助采样过程中，  
   $1-e^{-1} \simeq 63.2\%$ 的数据点将包含在训练集中，  
   这些数据点被称为 “袋内” 样本（另外36.8% 被称为 “袋外” 样本）
- **自助采样（bootstrapping）**
  - 类似于子采样，但是抽取样本是**有放回的**
  - 自助法已经不太常用，  
   因为训练数据中的重复观测在某些机器学习设置中会导致问题，  
   特别是与模型选择方法和嵌套重采样结合时  
   （因为在嵌套方案中，重复观测可能会同时出现在训练集和测试集中）

![image_4](./image_4.png)
> [三重交叉验证示意图](https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html)

### 构建重采样策略

- 查看所有重采样策略

```r
mlr_resamplings %>% as.data.table()
```

<table class='dataframe'>
<caption>A data.table: 9 x 4</caption>
<thead>
 <tr><th scope=col>key</th><th scope=col>label</th><th scope=col>params</th><th scope=col>iters</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
 <tr><td>bootstrap  </td><td>Bootstrap                    </td><td>ratio  , repeats</td><td> 30</td></tr>
 <tr><td>custom     </td><td>Custom Splits                </td><td></td><td> NA</td></tr>
 <tr><td>custom_cv  </td><td>Custom Split Cross-Validation</td><td></td><td> NA</td></tr>
 <tr><td>cv         </td><td>Cross-Validation             </td><td>folds</td><td> 10</td></tr>
 <tr><td>holdout    </td><td>Holdout                      </td><td>ratio</td><td>  1</td></tr>
 <tr><td>insample   </td><td>Insample Resampling          </td><td></td><td>  1</td></tr>
 <tr><td>loo        </td><td>Leave-One-Out                </td><td></td><td> NA</td></tr>
 <tr><td>repeated_cv</td><td>Repeated Cross-Validation    </td><td>folds  , repeats</td><td>100</td></tr>
 <tr><td>subsampling</td><td>Subsampling                  </td><td>ratio  , repeats</td><td> 30</td></tr>
</tbody>
</table>

- 创建`Resampling`对象

```r
rsmp('cv', folds = 5)
```

```css
[36m--[39m [1m[34m<ResamplingCV>[39m : Cross-Validation[22m [36m-------------------------------------------[39m
* Iterations: [34m5[39m
* Instantiated: [34mFALSE[39m
* Parameters: folds=5
```

- `Resampling`对象也有参数可以设定  
  - 可以用`Resampling$param_set`查看
  - 也能用`rsmp.cv$param_set$set_values()`重新设定参数

```r
rsmp.cv <- rsmp('cv')
rsmp.cv$param_set
```

```css
<ParamSet(1)>
       id    class lower upper nlevels        default  value
   <char>   <char> <int> <num>   <num>         <list> <list>
1:  folds ParamInt     2   Inf     Inf <NoDefault[0]>     10
```

```r
rsmp.cv$param_set$set_values(folds = 5)
rsmp.cv$param_set
```

```css
<ParamSet(1)>
       id    class lower upper nlevels        default  value
   <char>   <char> <int> <num>   <num>         <list> <list>
1:  folds ParamInt     2   Inf     Inf <NoDefault[0]>      5
```

- `Resampling$instantiate(Task)`可以手动构建重采样策略，  
  生成所有的训练集-测试集拆分

```r
rsmp.cv <- rsmp('cv', folds = 5)
rsmp.cv$instantiate(peng.tskC)

# 第一个训练集中前五个样本
rsmp.cv$train_set(1)[1:5]
# 第三个测试集中前五个样本
rsmp.cv$test_set(3)[1:5]
```

### 执行重采样

- 使用`resample()`执行重采样，参数依次为`Task`，`Learner`，`Resampling`
  - 注意，这里的`Learner`

```r
peng.lrnC.rpart.rr <- resample(peng.tskC, peng.lrnC.rpart, rsmp.cv)
peng.lrnC.rpart.rr
```

```css

[36m--[39m [1m[34m<ResampleResult>[39m with [34m5[39m resampling iterations[22m [36m-------------------------------[39m
  task_id    learner_id resampling_id iteration     prediction_test warnings
 penguins classif.rpart            cv         1 <PredictionClassif>        0
 penguins classif.rpart            cv         2 <PredictionClassif>        0
 penguins classif.rpart            cv         3 <PredictionClassif>        0
 penguins classif.rpart            cv         4 <PredictionClassif>        0
 penguins classif.rpart            cv         5 <PredictionClassif>        0
 errors
      0
      0
      0
      0
      0
```

- 默认是不储存所有训练出来的模型的，  
  如果要储存，则需要指定参数`store_models = TRUE`

```r
peng.lrnC.rpart.rr <- resample(peng.tskC, peng.lrnC.rpart, rsmp.cv, store_models = TRUE)
```

- 查看精准度评估结果

```r
# 分开的结果
peng.lrnC.rpart.rr$score(msr('classif.acc'))
# 整合结果
peng.lrnC.rpart.rr$aggregate(msr('classif.acc'))
```

<table class='dataframe'>
<caption>A rr_score: 5 x 9</caption>
<thead>
 <tr><th scope=col>task</th><th scope=col>task_id</th><th scope=col>learner</th><th scope=col>learner_id</th><th scope=col>resampling</th><th scope=col>resampling_id</th><th scope=col>iteration</th><th scope=col>prediction_test</th><th scope=col>classif.acc</th></tr>
 <tr><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>&lt;environment: 0x10a09dc58&gt;</td><td>penguins</td><td>&lt;environment: 0x10c5640f0&gt;</td><td>classif.rpart</td><td>&lt;environment: 0x118978278&gt;</td><td>cv</td><td>1</td><td>&lt;environment: 0x11964f5b8&gt;</td><td>0.9130435</td></tr>
 <tr><td>&lt;environment: 0x10a09dc58&gt;</td><td>penguins</td><td>&lt;environment: 0x139a15978&gt;</td><td>classif.rpart</td><td>&lt;environment: 0x118978278&gt;</td><td>cv</td><td>2</td><td>&lt;environment: 0x119767f20&gt;</td><td>0.9130435</td></tr>
 <tr><td>&lt;environment: 0x10a09dc58&gt;</td><td>penguins</td><td>&lt;environment: 0x1395c69e0&gt;</td><td>classif.rpart</td><td>&lt;environment: 0x118978278&gt;</td><td>cv</td><td>3</td><td>&lt;environment: 0x1099dbc90&gt;</td><td>0.9710145</td></tr>
 <tr><td>&lt;environment: 0x10a09dc58&gt;</td><td>penguins</td><td>&lt;environment: 0x1391f6740&gt;</td><td>classif.rpart</td><td>&lt;environment: 0x118978278&gt;</td><td>cv</td><td>4</td><td>&lt;environment: 0x11a3bea58&gt;</td><td>0.9710145</td></tr>
 <tr><td>&lt;environment: 0x10a09dc58&gt;</td><td>penguins</td><td>&lt;environment: 0x1380535f0&gt;</td><td>classif.rpart</td><td>&lt;environment: 0x118978278&gt;</td><td>cv</td><td>5</td><td>&lt;environment: 0x10d1c6e00&gt;</td><td>0.9411765</td></tr>
</tbody>
</table>

```css
classif.acc: 0.941858482523444
```

- 默认的整合方式是使用平均值，这里也可以指定其他方式
  - 这里用的是微平均（micro average）
    - 把重采样迭代中的预测结果合并到一个 `Prediction` 对象中，  
    然后直接在此对象上计算指标
  - 默认的聚合方法类型可以通过查询`Measure`对象的`$average`字段来找到

```r
peng.lrnC.rpart.rr$aggregate(msr('classif.acc', average = 'micro'))
```

```css
classif.acc: 0.941860465116279
```

```r
msr('classif.ce')$average
```

```css
'macro'
```

- 宏平均和微平均的详细

```r
# macro
mean(mean(c(3, 5, 9)), mean(c(1, 5)))

# micro
mean(c(3, 5, 9, 1, 5))
 ```

![image_5](./image_5.png)
> `$score()` 与 `$aggregate()` 之间差异的示例：前者在每次重采样迭代中将预测结果聚合为单个分数，而后者则在所有重采样迭代中聚合分数

- 可以用`autoplot.ResampleResult()`生成重采样的结果

```r
autoplot(peng.lrnC.rpart.rr, type = 'boxplot')
autoplot(peng.lrnC.rpart.rr, type = 'histogram')
```

### 重采样结果对象

- `ResampleResult$$prediction()`打印单个模型的预测结果，  
  这个结果是整合了全部模型预测的结果
  - 组合后的预测对象可用于手动计算微平均性能估计值
- `ResampleResult$$predictions()`分别打印所有模型的预测结果

```r
peng.lrnC.rpart.rr$prediction()
peng.lrnC.rpart.rr$predictions()
```

```css
[36m--[39m [1m[34m<PredictionClassif>[39m for [34m344[39m observations:[22m [36m-----------------------------------[39m
 row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
       7    Adelie    Adelie  0.98290598     0.01709402  0.00000000
       9    Adelie    Adelie  0.98290598     0.01709402  0.00000000
      16    Adelie    Adelie  0.98290598     0.01709402  0.00000000
     ---       ---       ---         ---            ---         ---
     320 Chinstrap Chinstrap  0.05454545     0.92727273  0.01818182
     338 Chinstrap Chinstrap  0.05454545     0.92727273  0.01818182
     344 Chinstrap Chinstrap  0.05454545     0.92727273  0.01818182
```

```css
[[1]]

[36m--[39m [1m[34m<PredictionClassif>[39m for [34m69[39m observations:[22m [36m------------------------------------[39m
 row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
       7    Adelie    Adelie  0.98290598     0.01709402  0.00000000
       9    Adelie    Adelie  0.98290598     0.01709402  0.00000000
      16    Adelie    Adelie  0.98290598     0.01709402  0.00000000
     ---       ---       ---         ---            ---         ---
     315 Chinstrap Chinstrap  0.05882353     0.92156863  0.01960784
     317 Chinstrap Chinstrap  0.28571429     0.57142857  0.14285714
     335 Chinstrap Chinstrap  0.05882353     0.92156863  0.01960784

[[2]]

[36m--[39m [1m[34m<PredictionClassif>[39m for [34m69[39m observations:[22m [36m------------------------------------[39m
 row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
       4    Adelie    Adelie  0.97580645     0.02419355    0.000000
      12    Adelie    Adelie  0.97580645     0.02419355    0.000000
      25    Adelie    Adelie  0.97580645     0.02419355    0.000000
     ---       ---       ---         ---            ---         ---
     334 Chinstrap Chinstrap  0.08000000     0.92000000    0.000000
     340 Chinstrap    Gentoo  0.00990099     0.02970297    0.960396
     343 Chinstrap    Gentoo  0.00990099     0.02970297    0.960396

[[3]]

[36m--[39m [1m[34m<PredictionClassif>[39m for [34m69[39m observations:[22m [36m------------------------------------[39m
 row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
       5    Adelie    Adelie   0.9669421     0.03305785  0.00000000
      10    Adelie    Adelie   0.9669421     0.03305785  0.00000000
      14    Adelie    Adelie   0.9669421     0.03305785  0.00000000
     ---       ---       ---         ---            ---         ---
     322 Chinstrap Chinstrap   0.0754717     0.90566038  0.01886792
     337 Chinstrap Chinstrap   0.2500000     0.50000000  0.25000000
     339 Chinstrap Chinstrap   0.0754717     0.90566038  0.01886792

[[4]]

[36m--[39m [1m[34m<PredictionClassif>[39m for [34m69[39m observations:[22m [36m------------------------------------[39m
 row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
       8    Adelie    Adelie  0.96124031     0.03875969   0.0000000
      21    Adelie    Adelie  0.96124031     0.03875969   0.0000000
      22    Adelie    Adelie  0.96124031     0.03875969   0.0000000
     ---       ---       ---         ---            ---         ---
     336 Chinstrap Chinstrap  0.06382979     0.91489362   0.0212766
     341 Chinstrap    Adelie  0.96124031     0.03875969   0.0000000
     342 Chinstrap Chinstrap  0.06382979     0.91489362   0.0212766

[[5]]

[36m--[39m [1m[34m<PredictionClassif>[39m for [34m68[39m observations:[22m [36m------------------------------------[39m
 row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
       1    Adelie    Adelie  0.96363636     0.03636364  0.00000000
       2    Adelie    Adelie  0.96363636     0.03636364  0.00000000
       3    Adelie    Adelie  0.96363636     0.03636364  0.00000000
     ---       ---       ---         ---            ---         ---
     320 Chinstrap Chinstrap  0.05454545     0.92727273  0.01818182
     338 Chinstrap Chinstrap  0.05454545     0.92727273  0.01818182
     344 Chinstrap Chinstrap  0.05454545     0.92727273  0.01818182
```

- 如果在重采样时指定了 `store_models = TRUE`，  
  也可以同理调出学习器
- `ResampleResult$learner`并不是聚合后的学习器，  
  而是**最终选择出来的模型**

```r
peng.lrnC.rpart.rr$learner
peng.lrnC.rpart.rr$learners
```

```css
[36m--[39m [1m[34m<LearnerClassifRpart>[39m (classif.rpart): Classification Tree[22m [36m------------------[39m
* Model: -
* Parameters: xval=0
* Packages: [34mmlr3[39m and [34mrpart[39m
* Predict Types: response and [prob]
* Feature Types: logical, integer, numeric, factor, and ordered
* Encapsulation: none (fallback: -)
* Properties: importance, missings, multiclass, selected_features, twoclass,
and weights
* Other settings: use_weights = 'use'
```

```css
[[1]]

[36m--[39m [1m[34m<LearnerClassifRpart>[39m (classif.rpart): Classification Tree[22m [36m------------------[39m
* Model: rpart
* Parameters: xval=0
* Packages: [34mmlr3[39m and [34mrpart[39m
* Predict Types: response and [prob]
* Feature Types: logical, integer, numeric, factor, and ordered
* Encapsulation: none (fallback: -)
* Properties: importance, missings, multiclass, selected_features, twoclass,
and weights
* Other settings: use_weights = 'use'

[[2]]

[36m--[39m [1m[34m<LearnerClassifRpart>[39m (classif.rpart): Classification Tree[22m [36m------------------[39m
* Model: rpart
* Parameters: xval=0
* Packages: [34mmlr3[39m and [34mrpart[39m
* Predict Types: response and [prob]
* Feature Types: logical, integer, numeric, factor, and ordered
* Encapsulation: none (fallback: -)
* Properties: importance, missings, multiclass, selected_features, twoclass,
and weights
* Other settings: use_weights = 'use'

[[3]]

[36m--[39m [1m[34m<LearnerClassifRpart>[39m (classif.rpart): Classification Tree[22m [36m------------------[39m
* Model: rpart
* Parameters: xval=0
* Packages: [34mmlr3[39m and [34mrpart[39m
* Predict Types: response and [prob]
* Feature Types: logical, integer, numeric, factor, and ordered
* Encapsulation: none (fallback: -)
* Properties: importance, missings, multiclass, selected_features, twoclass,
and weights
* Other settings: use_weights = 'use'

[[4]]

[36m--[39m [1m[34m<LearnerClassifRpart>[39m (classif.rpart): Classification Tree[22m [36m------------------[39m
* Model: rpart
* Parameters: xval=0
* Packages: [34mmlr3[39m and [34mrpart[39m
* Predict Types: response and [prob]
* Feature Types: logical, integer, numeric, factor, and ordered
* Encapsulation: none (fallback: -)
* Properties: importance, missings, multiclass, selected_features, twoclass,
and weights
* Other settings: use_weights = 'use'

[[5]]

[36m--[39m [1m[34m<LearnerClassifRpart>[39m (classif.rpart): Classification Tree[22m [36m------------------[39m
* Model: rpart
* Parameters: xval=0
* Packages: [34mmlr3[39m and [34mrpart[39m
* Predict Types: response and [prob]
* Feature Types: logical, integer, numeric, factor, and ordered
* Encapsulation: none (fallback: -)
* Properties: importance, missings, multiclass, selected_features, twoclass,
and weights
* Other settings: use_weights = 'use'
```

- 可以用`lapply()`查看每个模型中变量重要度

```r
# 查看全部
lapply(
 peng.lrnC.rpart.rr$learners[2:3], 
 function(x) return(x$model$variable.importance)
)

# 选择子集
lapply(
 peng.lrnC.rpart.rr$learners[2:3], 
 function(x) return(x$model$variable.importance)
)
```

### 自定义重采样

- `rsmp('custom')`用于构建**留出重采样**
  - 留出重采样需要手动构建样本ID

```r
peng.rs.ho <- rsmp('custom')
peng.rs.ho.trainSets <- c(1:5, 153:158, 277:280)
peng.rs.ho$instantiate(
 peng.tskC,
 train = list(peng.rs.ho.trainSets, peng.rs.ho.trainSets + 5),
 test = list(peng.rs.ho.trainSets + 15, peng.rs.ho.trainSets + 25)
)
resample(peng.tskC, peng.lrnC.rpart, peng.rr.ho)$prediction()
```

```css
[36m--[39m [1m[34m<PredictionClassif>[39m for [34m30[39m observations:[22m [36m------------------------------------[39m
 row_ids     truth response prob.Adelie prob.Chinstrap prob.Gentoo
      16    Adelie   Gentoo   0.3333333      0.2666667         0.4
      17    Adelie   Gentoo   0.3333333      0.2666667         0.4
      18    Adelie   Gentoo   0.3333333      0.2666667         0.4
     ---       ---      ---         ---            ---         ---
     303 Chinstrap   Gentoo   0.3333333      0.2666667         0.4
     304 Chinstrap   Gentoo   0.3333333      0.2666667         0.4
     305 Chinstrap   Gentoo   0.3333333      0.2666667         0.4
```

- `rsmp('custom_cv')`用于构建**CV重采样**

```r
# 仅选用4个样本，分别是第1，100，200，300行
peng.tskC_small <- peng.tskC$filter(c(1, 100, 200, 300))
peng.rs.ccv <- rsmp('custom_cv')
# 第1，3行为训练集，2，4行为测试集
peng.rs.ccv.folds <- as.factor(c(1, 2, 1, 2))
peng.rs.ccv$instantiate(tskC_small, f = peng.rs.ccv.folds)
resample(peng.tskC_small, peng.lrnC.rpart, peng.rs.ccv)$predictions()
```

```css
Error in frame$yval2[where, 1L + nclass + 1L:nclass, drop = FALSE]: subscript out of bounds
```

- 由于上面莫名其妙地报错，所以用了个新的

```r
# `$filter`是就地修改，如果改了的话运行：
# peng.tskC <- tsk('penguins')
peng.tskC2 <- peng.tskC$clone()
peng.rs.ccv <- rsmp('custom_cv')
peng.rs.ccv.folds <- as.factor(c(rep(1,244),rep(2,100)))
peng.rs.ccv$instantiate(peng.tskC2, f = peng.rs.ccv.folds)
resample(peng.tskC2, peng.lrnC.rpart, peng.rs.ccv)$predictions()
```

```css
[[1]]

[36m--[39m [1m[34m<PredictionClassif>[39m for [34m244[39m observations:[22m [36m-----------------------------------[39m
 row_ids  truth  response prob.Adelie prob.Chinstrap prob.Gentoo
       1 Adelie Chinstrap           0              1           0
       2 Adelie Chinstrap           0              1           0
       3 Adelie Chinstrap           0              1           0
     ---    ---       ---         ---            ---         ---
     242 Gentoo    Gentoo           0              0           1
     243 Gentoo    Gentoo           0              0           1
     244 Gentoo    Gentoo           0              0           1

[[2]]

[36m--[39m [1m[34m<PredictionClassif>[39m for [34m100[39m observations:[22m [36m-----------------------------------[39m
 row_ids     truth response prob.Adelie prob.Chinstrap prob.Gentoo
     245    Gentoo   Gentoo   0.0212766              0   0.9787234
     246    Gentoo   Gentoo   0.0212766              0   0.9787234
     247    Gentoo   Gentoo   0.0212766              0   0.9787234
     ---       ---      ---         ---            ---         ---
     342 Chinstrap   Adelie   1.0000000              0   0.0000000
     343 Chinstrap   Gentoo   0.0212766              0   0.9787234
     344 Chinstrap   Adelie   1.0000000              0   0.0000000
```

### 重采样中的分组与分层

#### 分组重采样

- 在一些特定的情形下，分组是至关重要的：
  - 纵向研究中，对同一个个体进行多个时间点测量，  
   如果不分组，可能会高估模型的泛化，  
   因为同一个个体的观测值可能同时出现在训练集和测试集中

```r
peng.tskC <-  tsk('penguins')
peng.tskC$set_col_roles('year', 'group')

peng.rs.loo <- rsmp('loo')
peng.rs.loo$instantiate(peng.tskC)

table(peng.tskC$data(rows = peng.rs.loo$train_set(1), cols = 'year'))
table(peng.tskC$data(rows = peng.rs.loo$test_set(1), cols = 'year'))
```

```css
year
2007 2008 
 110  114 

year
2009 
 120 
```

- 用CV试了下

```r
peng.tskC <-  tsk('penguins')
peng.tskC$set_col_roles('year', 'group')

peng.rs.cv <- rsmp('cv')
peng.rs.cv$instantiate(peng.tskC)

table(peng.tskC$data(rows = peng.rs.cv$train_set(1), cols = 'year'))
table(peng.tskC$data(rows = peng.rs.cv$test_set(1), cols = 'year'))
```

```css
year
2008 2009 
 114  120 

year
2007 
 110 
```

- 上面可以看得出，  
  2008，2009年的数据分配到了**训练集**，  
  而2007的数据分配到了**测试集**
  - 所以`factor`的第一级会分配到测试集中

### 分层重采样

- 分层能够确保在训练集与测试集中样本的**特征分布是相同**的，  
  以保证泛化能力较小的偏差
- 当不平衡情况严重时，少数类**可能根本不会出现**在训练集中
  - 因此，这些重采样迭代中的中间模型将永远不会预测缺失的类，  
   从而导致对任何没有分层的重采样策略的性能估计产生误导
- 依然用`Task$set_col_roles()`指定

```r
peng.tskC <-  tsk('penguins')
peng.tskC$set_col_roles('species', c('target', 'stratum'))

peng.rs.cv <- rsmp('cv', folds = 10)
peng.rs.cv$instantiate(peng.tskC)

fold1 = prop.table(table(peng.tskC$data(rows = peng.rs.cv$test_set(1),
  cols = 'species')))
fold2 = prop.table(table(peng.tskC$data(rows = peng.rs.cv$test_set(2),
  cols = 'species')))

rbind('Fold 1' = fold1, 'Fold 2' = fold2)
```

<table class='dataframe'>
<caption>A matrix: 2 x 3 of type dbl</caption>
<thead>
 <tr><th></th><th scope=col>Adelie</th><th scope=col>Chinstrap</th><th scope=col>Gentoo</th></tr>
</thead>
<tbody>
 <tr><th scope=row>Fold 1</th><td>0.4444444</td><td>0.1944444</td><td>0.3611111</td></tr>
 <tr><th scope=row>Fold 2</th><td>0.4444444</td><td>0.1944444</td><td>0.3611111</td></tr>
</tbody>
</table>

## 基准测试

### `benchmark()`

- `benchmark()`可以执行在每个`Task`和`Learner`上的重采样`resample()`，  
  然后手机它们的结果
- 首先调用`benchmark_grid()`函数构建一个  
  不同`Task`，`Learner`，和`Resampling`的策略组合
- （懒得改变量名了，就用教程给的）

```r
# 这里`tasks`是一个`list`
# 使用本地文件可以先`as_task_*()`构建好任务，再用`list()`列表化
tasks <- tsks(c('german_credit', 'sonar')) 
learners <- lrns(c('classif.rpart', 'classif.ranger',
  'classif.featureless'), predict_type = 'prob')
rsmp_cv5 <- rsmp('cv', folds = 5)

design <- benchmark_grid(tasks, learners, rsmp_cv5)
```

- 本来用`head()`可以看的，但是在VS code的 jupyter 里好像用不了，  
  所以换了种`print()`的方式

```r
design %>% head()
```

<table class='dataframe'>
<caption>A benchmark_grid: 6 x 3</caption>
<thead>
 <tr><th scope=col>task</th><th scope=col>learner</th><th scope=col>resampling</th></tr>
 <tr><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>&lt;environment: 0x14a4fe818&gt;</td><td>&lt;environment: 0x13ad80320&gt;</td><td>&lt;environment: 0x10b13e698&gt;</td></tr>
 <tr><td>&lt;environment: 0x14a4fe818&gt;</td><td>&lt;environment: 0x139489040&gt;</td><td>&lt;environment: 0x10b13e698&gt;</td></tr>
 <tr><td>&lt;environment: 0x14a4fe818&gt;</td><td>&lt;environment: 0x10df71690&gt;</td><td>&lt;environment: 0x10b13e698&gt;</td></tr>
 <tr><td>&lt;environment: 0x14c562668&gt;</td><td>&lt;environment: 0x13ad80320&gt;</td><td>&lt;environment: 0x10fe429c0&gt;</td></tr>
 <tr><td>&lt;environment: 0x14c562668&gt;</td><td>&lt;environment: 0x139489040&gt;</td><td>&lt;environment: 0x10fe429c0&gt;</td></tr>
 <tr><td>&lt;environment: 0x14c562668&gt;</td><td>&lt;environment: 0x10df71690&gt;</td><td>&lt;environment: 0x10fe429c0&gt;</td></tr>
</tbody>
</table>

```r
print(design, cols =ncol(design))
```

```css
            task             learner resampling
          <char>              <char>     <char>
1: german_credit       classif.rpart         cv
2: german_credit      classif.ranger         cv
3: german_credit classif.featureless         cv
4:         sonar       classif.rpart         cv
5:         sonar      classif.ranger         cv
6:         sonar classif.featureless         cv
```

- 这里可以看到`design`就是一个`data.table`，  
  生成的原理也类似于`expand_grid()`
- 内容也可以手动进行修改

- 默认情况下，`benchmark_grid()` 会在任务上实例化重采样，  
  这意味着会生成具体的训练 - 测试分割
  - 由于此过程是随机的，因此有必要在调用 `benchmark_grid()` 之前  
   **设置随机种子**，以确保数据分割的可重复性

- 在`benchmark_grid()`创建基准测试设计后，  
  将设计表格传递给`benchmark()`

```r
bmr <- benchmark(design)
bmr
```

```css
[36m--[39m [1m[34m<BenchmarkResult>[39m of [34m30[39m rows with [34m6[39m resampling run[22m [36m--------------------------[39m
 nr       task_id          learner_id resampling_id iters warnings errors
  1 german_credit       classif.rpart            cv     5        0      0
  2 german_credit      classif.ranger            cv     5        0      0
  3 german_credit classif.featureless            cv     5        0      0
  4         sonar       classif.rpart            cv     5        0      0
  5         sonar      classif.ranger            cv     5        0      0
  6         sonar classif.featureless            cv     5        0      0
```

- 可以用`BenchmarkResult$score()`查看每个学习器/任务/重采样组合的每一折的结果，  
  用`BenchmarkResult$aggregate()`查看每个学习器/任务/重采样组合的每一折的结果
- 注意，这里的`x.[rowVector,.(var1,var2,...)]`用法是`data.table`的独有用法

```r
bmr$score()[c(1, 7, 13), .(iteration, task_id, learner_id, classif.ce)]
```

<table class='dataframe'>
<caption>A bmr_score: 3 x 4</caption>
<thead>
 <tr><th scope=col>iteration</th><th scope=col>task_id</th><th scope=col>learner_id</th><th scope=col>classif.ce</th></tr>
 <tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>1</td><td>german_credit</td><td>classif.rpart      </td><td>0.29</td></tr>
 <tr><td>2</td><td>german_credit</td><td>classif.ranger     </td><td>0.25</td></tr>
 <tr><td>3</td><td>german_credit</td><td>classif.featureless</td><td>0.31</td></tr>
</tbody>
</table>

```r
bmr$aggregate()[, .(task_id, learner_id, classif.ce)]
```

<table class='dataframe'>
<caption>A bmr_aggregate: 6 x 3</caption>
<thead>
 <tr><th scope=col>task_id</th><th scope=col>learner_id</th><th scope=col>classif.ce</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>german_credit</td><td>classif.rpart      </td><td>0.2720000</td></tr>
 <tr><td>german_credit</td><td>classif.ranger     </td><td>0.2420000</td></tr>
 <tr><td>german_credit</td><td>classif.featureless</td><td>0.3000000</td></tr>
 <tr><td>sonar        </td><td>classif.rpart      </td><td>0.2792102</td></tr>
 <tr><td>sonar        </td><td>classif.ranger     </td><td>0.1782811</td></tr>
 <tr><td>sonar        </td><td>classif.featureless</td><td>0.4670151</td></tr>
</tbody>
</table>

- 这里也可以改成其他的评估器

```r
bmr$aggregate(msr('classif.acc'))[, .(task_id, learner_id, classif.acc)]
```

<table class='dataframe'>
<caption>A bmr_aggregate: 6 x 3</caption>
<thead>
 <tr><th scope=col>task_id</th><th scope=col>learner_id</th><th scope=col>classif.acc</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>german_credit</td><td>classif.rpart      </td><td>0.7280000</td></tr>
 <tr><td>german_credit</td><td>classif.ranger     </td><td>0.7580000</td></tr>
 <tr><td>german_credit</td><td>classif.featureless</td><td>0.7000000</td></tr>
 <tr><td>sonar        </td><td>classif.rpart      </td><td>0.7207898</td></tr>
 <tr><td>sonar        </td><td>classif.ranger     </td><td>0.8217189</td></tr>
 <tr><td>sonar        </td><td>classif.featureless</td><td>0.5329849</td></tr>
</tbody>
</table>

### `BenchmarkResult`对象

- `BenchmarkResult`对象其实就是多个`ResampleResult`对象的集合

```r
bmrdt <- as.data.table(bmr)
bmrdt[1:2, .(task, learner, resampling, iteration)]
```

<table class='dataframe'>
<caption>A data.table: 2 x 4</caption>
<thead>
 <tr><th scope=col>task</th><th scope=col>learner</th><th scope=col>resampling</th><th scope=col>iteration</th></tr>
 <tr><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;int&gt;</th></tr>
</thead>
<tbody>
 <tr><td>&lt;environment: 0x164aae688&gt;</td><td>&lt;environment: 0x14c201600&gt;</td><td>&lt;environment: 0x13ff88320&gt;</td><td>1</td></tr>
 <tr><td>&lt;environment: 0x164aae688&gt;</td><td>&lt;environment: 0x14a63e1b8&gt;</td><td>&lt;environment: 0x13ff88320&gt;</td><td>2</td></tr>
</tbody>
</table>

- 存储的 `ResampleResult` 可以通过 `ResampleResult$resample_result(i)` 方法提取
  - `i`是执行的重采样实验的索引

```r
rr1 <- bmr$resample_result(1)
rr2 <- bmr$resample_result(2)

rr1
rr2
```

```css
[36m--[39m [1m[34m<ResampleResult>[39m with [34m5[39m resampling iterations[22m [36m-------------------------------[39m
       task_id    learner_id resampling_id iteration     prediction_test
 german_credit classif.rpart            cv         1 <PredictionClassif>
 german_credit classif.rpart            cv         2 <PredictionClassif>
 german_credit classif.rpart            cv         3 <PredictionClassif>
 german_credit classif.rpart            cv         4 <PredictionClassif>
 german_credit classif.rpart            cv         5 <PredictionClassif>
 warnings errors
        0      0
        0      0
        0      0
        0      0
        0      0


[36m--[39m [1m[34m<ResampleResult>[39m with [34m5[39m resampling iterations[22m [36m-------------------------------[39m
       task_id     learner_id resampling_id iteration     prediction_test
 german_credit classif.ranger            cv         1 <PredictionClassif>
 german_credit classif.ranger            cv         2 <PredictionClassif>
 german_credit classif.ranger            cv         3 <PredictionClassif>
 german_credit classif.ranger            cv         4 <PredictionClassif>
 german_credit classif.ranger            cv         5 <PredictionClassif>
 warnings errors
        0      0
        0      0
        0      0
        0      0
        0      0
```

- `ResampleResult$resample_results()`也可以打印所有的重采样

```r
bmr$resample_results
```

<table class='dataframe'>
<caption>A data.table: 6 x 3</caption>
<thead>
 <tr><th scope=col>uhash</th><th scope=col>nr</th><th scope=col>resample_result</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>e3e12f28-dbeb-491f-9053-bf542a1a6eb7</td><td>1</td><td>&lt;environment: 0x13ae1b7c0&gt;</td></tr>
 <tr><td>01b8d3a8-9d06-4520-b3d9-a43dfe6eb620</td><td>2</td><td>&lt;environment: 0x13ae79ba0&gt;</td></tr>
 <tr><td>f859c5f2-80ba-4b18-8074-9fa3007c8e16</td><td>3</td><td>&lt;environment: 0x13ab7e918&gt;</td></tr>
 <tr><td>c844f002-4add-428c-9bc0-6f1cd7439ffb</td><td>4</td><td>&lt;environment: 0x148f66558&gt;</td></tr>
 <tr><td>bba8375a-f30e-4b44-8caa-c7b572d5b151</td><td>5</td><td>&lt;environment: 0x148ff9b30&gt;</td></tr>
 <tr><td>78cfe007-fbfd-4f2d-bcd2-3fba990295b5</td><td>6</td><td>&lt;environment: 0x13a9f3368&gt;</td></tr>
</tbody>
</table>

- `as_benchmark_result()` 可用于将对象从 `ResampleResult` 转换为 `BenchmarkResult`
- `c()` 方法可用于合并多个 `BenchmarkResult` 对象
  - 这在多台机器上进行实验时可能会很有用

```r
bmr1 <- as_benchmark_result(rr1)
bmr2 <- as_benchmark_result(rr2)

c(bmr1, bmr2)
```

```css
[36m--[39m [1m[34m<BenchmarkResult>[39m of [34m10[39m rows with [34m2[39m resampling run[22m [36m--------------------------[39m
 nr       task_id     learner_id resampling_id iters warnings errors
  1 german_credit  classif.rpart            cv     5        0      0
  2 german_credit classif.ranger            cv     5        0      0
```

### 可视化`BenchmarkResult`的评估结果

```r
autoplot(bmr, measure = msr('classif.acc'))
```

## 二元分类器的评估

### 混淆矩阵（Confusion Matrix）

- 基于混淆矩阵，可以计算以下指标
  - True Positive Rate (TPR), Sensitivity or Recall
  - True Negative Rate (TNR) or Specificity
  - False Positive Rate (FPR), or 1- Specificity
  - Positive Predictive Value (PPV) or Precision
  - Negative Predictive Value (NPV)
  - Accuracy (ACC)
  - F1-score
   $$2 \times \frac{Precision \times Recall}{Precision + Recall}$$

![image_6](./image_6.png)

- `mlr3measures::confusion_matrix()`可以生成混淆矩阵

```r
# 准备一下数据
gc.tskC <- tsk('german_credit')
gc.tskC.splits <- partition(gc.tskC)
gc.lrnC.ranger <- lrn('classif.ranger', predict_type = 'prob')
gc.lrnC.ranger$train(gc.tskC, row_ids = gc.tskC.splits$train)
gc.lrnC.ranger.pred <- gc.lrnC.ranger$predict(gc.tskC, row_ids = gc.tskC.splits$test)

# 混淆矩阵
gc.cm.ranger <- mlr3measures::confusion_matrix(
    truth = gc.lrnC.ranger.pred$truth,
    response = gc.lrnC.ranger.pred$response, 
    positive = gc.tskC$positive
)
gc.cm.ranger
```

```css
        truth
response good bad
    good  207  60
    bad    26  37
acc :  0.7394; ce  :  0.2606; dor :  4.9096; f1  :  0.8280 
fdr :  0.2247; fnr :  0.1116; fomr:  0.4127; fpr :  0.6186 
mcc :  0.3128; npv :  0.5873; ppv :  0.7753; tnr :  0.3814 
tpr :  0.8884 
```

### 受试者工作特征曲线（ROC curve）分析

- 对于二元分类问题最常用的评估器
- 由不同阈值进行分类截断时的FPR与TPR构成的一条曲线

- ROC曲线可以由`autoplot(Prediction)`生成
  - **注意**：`Learner <- lrn('classif.*', predict_type = 'prob')`  
   必须指定预测的类型为`'prob'`

```r
autoplot(gc.lrnC.ranger.pred, type = 'roc')
```

- 使用`msr('classif.auc')`调用评估器AUC分析

```r
gc.lrnC.ranger.pred$score(msr('classif.auc'))
```

```css
classif.auc: 0.772628921745402
```

### 精确率 - 召回率曲线（PRC）

- PRC能够可视化阳性预测值/精确率与真阳性率/召回率的关系
- PRC忽略了真阴性的数量，这在不平衡总体中很有用，  
  因为阳性类别总是少见的，因此具有高真阳性率的分类器可能仍然提供的信息不多

```r
autoplot(gc.lrnC.ranger.pred, type = 'prc')
```

- 使用`msr('classif.prauc')`调用评估器PRC AUC分析

```r
gc.lrnC.ranger.pred$score(msr('classif.prauc'))
```

```css
classif.prauc: 0.891557073290125
```

### 其他的可视化

- 可视化其他性能指标随阈值变化的关系
  - 这里是误报率（FPR）和准确率为例

```r
autoplot(gc.lrnC.ranger.pred, type = 'threshold', measure = msr('classif.fpr'))
autoplot(gc.lrnC.ranger.pred, type = 'threshold', measure = msr('classif.acc'))
```

- 上述的可视化方法同样适用于`ResampleResult`对象

## 基本调参方法

### 调参的整体流程

![image_7](./image_7.png)

### 模型调优（Model Tuning）

- **搜索空间（search space）/调优空间（tuning space）**
  - 可供调整的超参集合
- **不可调超参数**
  - 少数模型的训练函数提供了和训练模型不相干的超参
  - 例如`lrn('classif.ranger')` 中的 `verbose` 控制训练期间向用户显示多少信息

- 对于数值型的超参，需要指定调整的范围
- 使用`to_tune()`来构建调整范围的下限与上限

```r
learner <- lrn('classif.svm',
  type  = 'C-classification',
  kernel = 'radial',
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1)
)
learner
```

- 如果对用`to_tune()`标记了超参的学习器调用`Learner$train()`的话会抛出错误

### 终止器（Terminator）

- ~~终结者~~
- `mlr_terminators`存储了所有终止器的种类，  
  使用 `trm()` 创建终止器

| Terminator <br>终止器             | Function call and default parameters <br>函数调用和默认参数 |
| ------------------------------ | -------------------------------------------------- |
| Clock Time <br>时钟时间            | `trm('clock_time')`                                |
| Combo <br>组合                   | `trm('combo', any = TRUE)`                         |
| None <br>无                     | `trm('none')`                                      |
| Number of Evaluations <br>评估次数 | `trm('evals', n_evals = 100, k = 0)`               |
| Performance Level <br>性能水平     | `trm('perf_reached', level = 0.1)`                 |
| Run Time <br>运行时间              | `trm('run_time', secs = 30)`                       |
| Stagnation <br>停滞              | `trm('stagnation', iters = 10, threshold = 0)`     |

```r
mlr_terminators %>% as.data.table()
```

<table class='dataframe'>
<caption>A data.table: 9 x 4</caption>
<thead>
 <tr><th scope=col>key</th><th scope=col>label</th><th scope=col>properties</th><th scope=col>unit</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
 <tr><td>clock_time            </td><td>Clock Time            </td><td>single-crit, multi-crit </td><td>seconds    </td></tr>
 <tr><td>combo                 </td><td>Combination           </td><td>single-crit, multi-crit </td><td>percent    </td></tr>
 <tr><td>evals                 </td><td>Number of Evaluation  </td><td>single-crit, multi-crit </td><td>evaluations</td></tr>
 <tr><td>none                  </td><td>None                  </td><td>single-crit, multi-crit </td><td>percent    </td></tr>
 <tr><td>perf_reached          </td><td>Performance Level     </td><td>single-crit</td><td>percent    </td></tr>
 <tr><td>run_time              </td><td>Run Time              </td><td>single-crit, multi-crit </td><td>seconds    </td></tr>
 <tr><td>stagnation            </td><td>Stagnation            </td><td>single-crit</td><td>percent    </td></tr>
 <tr><td>stagnation_batch      </td><td>Stagnation Batch      </td><td>single-crit</td><td>percent    </td></tr>
 <tr><td>stagnation_hypervolume</td><td>Stagnation Hypervolume</td><td>multi-crit</td><td>percent    </td></tr>
</tbody>
</table>

- 最常用的终止条件是在特定时间（`trm('run_time'`），  
  或给定评估次数（`trm('evals')`）后停止调优
  - 选择运行时间通常基于实际考虑和直觉
    - 在计算集群上，使用时间限制可能很重要，  
    因为可能需要为计算任务指定最长运行时间
  - `trm('perf_reached')`在达到指定性能水平时停止调优
    - 如果某个性能水平被认为足以满足模型的实际应用，这会很有帮助
    - 但是，如果设置得过于乐观，调优可能永远不会终止
- `trm('stagnation')`在设定数量的`iterations`中没有取得大于`threshold`的进展时停止
  - 对于复杂的搜索空间，阈值可能很难选择
  - 因为尽管仍有（可能很大的）改进空间，但优化可能会过早停止
- `trm('none')`用于自行控制终止的调谐器，因此此终止条件不起作用
- 这些终止条件中的任何一个都可以通过使用`trm('combo')`自由组合，  
  可用于指定超参数优化在任何终止条件触发时完成（`any = TRUE`），  
  或者在所有终止条件触发时完成（`any = FALSE`）

### 使用`ti()`进行实例调优（Tuning Instance）

- 这里用`ti()`创建了一个单目标调优的`TuningInstanceBatchSingleCrit`对象

```r
# 调取预设任务`'sonar'`
tsk_sonar <-  tsk('sonar')
# 构建学习器
learner <-  lrn('classif.svm',
 # 指定调优的超参`cose`和`gamma`
 cost  = to_tune(1e-1, 1e5),
 gamma = to_tune(1e-1, 1),
 # 设置其他的超参
 kernel = 'radial',
 type = 'C-classification'
)
# 进行实例调优
instance <-  ti(
 task = tsk_sonar,
 learner = learner,
 resampling = rsmp('cv', folds = 3),
 # 这里只有一个`Measure`，所以是单目标调优
 # 如果这里是`msrs(c('msr1','msr2'))`，那么就是多目标调优
 measures = msr('classif.ce'),
 # 遍历整个网格，不过早停止，因此不设定终止器
 terminator = trm('none')
)

instance
```

```css
<TuningInstanceBatchSingleCrit>
* State:  Not optimized
* Objective: <ObjectiveTuningBatch:classif.svm_on_sonar>
* Search Space:
       id    class lower upper nlevels
   <char>   <char> <num> <num>   <num>
1:   cost ParamDbl   0.1 1e+05     Inf
2:  gamma ParamDbl   0.1 1e+00     Inf
* Terminator: <TerminatorNone>
```

- 其实这里`<TuningInstanceBatchSingleCrit>`已经看得出是单目标调优对象了
- 可以随便看看`instance`的类

```r
instance %>% class()
```

```css
'TuningInstanceBatchSingleCrit''OptimInstanceBatchSingleCrit''OptimInstanceBatch''OptimInstance''R6'
```

### 调优器（Tuner）

#### 调优器种类

- 下表给出了可用的`Tuner`

| Tuner <br>调谐器                                | Function call <br>函数调用 | Package <br>包   |
| -------------------------------------------- | ---------------------- | --------------- |
| Random Search <br>随机搜索                       | `tnr('random_search')` | `mlr3tuning`    |
| Grid Search <br>网格搜索                         | `tnr('grid_search')`   | `mlr3tuning`    |
| Bayesian Optimization <br>贝叶斯优化              | `tnr('mbo')`           | `mlr3mbo`       |
| CMA-ES <br>协方差矩阵自适应进化策略                      | `tnr('cmaes')`         | `adagio`        |
| Iterated Racing <br>迭代竞赛                     | `tnr('irace')`         | `irace`         |
| Hyperband <br>超参数优化算法                        | `tnr('hyperband')`     | `mlr3hyperband` |
| Generalized Simulated Annealing <br>广义模拟退火算法 | `tnr('gensa')`         | `GenSA`         |
| Nonlinear Optimization <br>非线性优化             | `tnr('nloptr')`        | `nloptr`        |

- 使用`tnr()`来调用调优器

```r
tnr('grid_search')
```

```css
<TunerBatchGridSearch>: Grid Search
* Parameters: batch_size=1, resolution=10
* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct
* Properties: dependencies, single-crit, multi-crit
* Packages: mlr3tuning, bbotk
```

#### 调优策略

- **搜索策略（Search strategies）**
  - 网格搜索和随机搜索是最基本的算法，在初始实验中通常首先被选用
  - **网格搜索**
    - 思路是详尽地评估给定超参数值的每一种可能组合
    - 分类超参通常会在其所有可能取值上进行评估
    - 数值型和整型超参数值会根据给定的分辨率  
    （即每个超参数要尝试的不同值的数量），  
    在其边界约束（上限和下限）内等距分布
  - **随机搜索**
    - 从预先指定的分布（通常是均匀分布）中为每个超参数独立地随机选择值
  - 这两种方法都不具备适应性，  
   也就是说每次提出的配置都忽略了先前配置的性能
  - 由于其简单性，  
   网格搜索和随机搜索都可以处理混合搜索空间
   （即超参数可以是数值型、整型或分类型）以及分层搜索空间

- **自适应算法（Adaptive Algorithms）**
  - 自适应算法从**先前评估**的配置中学习，以快速找到良好的配置
  - **贝叶斯优化（Bayesian Optimization）**
    - 使用**代理模型**来近似待优化的未知函数
      - 在超参优化中，这将是从超参配置到估计的泛化性能的映射
    - 如果选择了合适的代理模型，例如随机森林，  
    贝叶斯优化可以相当灵活，甚至可以处理混合和分层搜索空间
  - **协方差矩阵自适应进化策略**  
   **（Covariance Matrix Adaptation Evolution Strategy）**
    - 在候选点上维持一个概率分布，该分布由一个均值向量和协方差矩阵表示  
    通过从这个分布中采样生成一组新的候选点，每个候选点被选中的概率与其性能成正比
    - 协方差矩阵会随着时间进行调整，以反映性能态势
    - 通过`miesmuschel`包，在mlr3中还可使用更多进化策略
  - **迭代竞赛算法（Iterated Racing）**
    - 迭代舍弃那些经统计检验显示性能不佳的配置
    - 首先从参数化密度中随机采样的初始配置群体中进行 “竞赛”，  
    然后利用竞赛中幸存的配置来随机更新后续竞赛的密度，  
    以便聚焦于搜索空间中有前景的区域，  
    依此类推
  - **超带算法（Hyperband）/ 多保真度超参数优化（Multi-fidelity HPO）**
    - 一种自适应方法，  
    利用计算成本较低的低保真度评估的预测能力  
    （即质量较差的预测例如那些由训练轮数较少的神经网络产生的预测）  
    来提高整体优化效率

#### 策略选择

- **搜索策略**
  - 如果搜索空间**较小**或结构**不复杂**，  
   网格搜索或许能够在合理时间内详尽评估整个搜索空间
  - 由于高纬度的搜索空间导致的的**维度灾难**以及对数值**搜索空间的覆盖不足**，  
   通常**不推荐使用网格搜索**
    - 此时，**随机搜索**往往是更好的选择，  
    因为可以考虑更多独特值
- **更优的算法**
  - 适用于高维度或结构更复杂的搜索空间
  - **进化策略和贝叶斯优化**间的选择
    - 根据函数评估的成本进行选择
    - **进化策略（evolutionary strategies）**
      - 超参配置能够快速评估时
    - **贝叶斯优化（Bayesian optimization）**
      - 模型评估耗时且优化预算有限时
    - 通常推荐在超参优化中使用**贝叶斯优化**
      - 虽然贝叶斯优化的优化成本相对较大  
     （例如，在每次迭代中，都要训练代理模型并优化采集函数），  
     但在诸如机器学习模型重采样等函数评估成本相对较高的情况下，  
     这种影响较小
  - 在超参数优化问题涉及有意义的**保真度参数**（例如，轮数、树的数量、提升轮数）  
   且需要**高效使用优化预算**的情况下，  
   像Hyperband这样的多保真度超参数优化算法可能值得考虑

#### 调优器处理的类型

- `Learner$param_classes`
  - 提供可以处理哪些类别的超参数
- `Learner$param_classes`
  - 提供可以处理哪些特性

```r
tnr('grid_search')$param_classes
tnr('grid_search')$properties
```

```css
'ParamLgl''ParamInt''ParamDbl''ParamFct'
'dependencies''single-crit''multi-crit'
```

- （重新回顾一下如何去看超参类型）
  - 这里的`class`就是了

```r
lrn('classif.svm')$param_set
```

```css
<ParamSet(16)>
Key: <id>
                 id    class lower upper nlevels          default parents
             <char>   <char> <num> <num>   <num>           <list>  <list>
 1:       cachesize ParamDbl  -Inf   Inf     Inf               40  [NULL]
 2:   class.weights ParamUty    NA    NA     Inf           [NULL]  [NULL]
 3:           coef0 ParamDbl  -Inf   Inf     Inf                0  kernel
 4:            cost ParamDbl     0   Inf     Inf                1    type
 5:           cross ParamInt     0   Inf     Inf                0  [NULL]
 6: decision.values ParamLgl    NA    NA       2            FALSE  [NULL]
 7:          degree ParamInt     1   Inf     Inf                3  kernel
 8:         epsilon ParamDbl     0   Inf     Inf              0.1  [NULL]
 9:          fitted ParamLgl    NA    NA       2             TRUE  [NULL]
10:           gamma ParamDbl     0   Inf     Inf   <NoDefault[0]>  kernel
11:          kernel ParamFct    NA    NA       4           radial  [NULL]
12:              nu ParamDbl  -Inf   Inf     Inf              0.5    type
13:           scale ParamUty    NA    NA     Inf             TRUE  [NULL]
14:       shrinking ParamLgl    NA    NA       2             TRUE  [NULL]
15:       tolerance ParamDbl     0   Inf     Inf            0.001  [NULL]
16:            type ParamFct    NA    NA       2 C-classification  [NULL]
     value
    <list>
 1: [NULL]
 2: [NULL]
 3: [NULL]
 4: [NULL]
 5: [NULL]
 6: [NULL]
 7: [NULL]
 8: [NULL]
 9: [NULL]
10: [NULL]
11: [NULL]
12: [NULL]
13: [NULL]
14: [NULL]
15: [NULL]
16: [NULL]
```

```r
lrn('classif.svm')$param_set$deps
```

<table class='dataframe'>
<caption>A data.table: 5 x 3</caption>
<thead>
 <tr><th scope=col>id</th><th scope=col>on</th><th scope=col>cond</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>coef0 </td><td>kernel</td><td>polynomial    , sigmoid       , %s %%in%% {%s}</td></tr>
 <tr><td>cost  </td><td>type  </td><td>C-classification, %s == %s        </td></tr>
 <tr><td>degree</td><td>kernel</td><td>polynomial, %s == %s  </td></tr>
 <tr><td>gamma </td><td>kernel</td><td>polynomial    , radial        , sigmoid       , %s %%in%% {%s}</td></tr>
 <tr><td>nu    </td><td>type  </td><td>nu-classification, %s == %s         </td></tr>
</tbody>
</table>

#### 触发调优过程

- 这个是之前构建好的实例

```r
tsk_sonar <-  tsk('sonar')
# 构建学习器
learner <-  lrn('classif.svm',
 # 指定调优的超参`cose`和`gamma`
 cost  = to_tune(1e-1, 1e5),
 gamma = to_tune(1e-1, 1),
 # 设置其他的超参
 kernel = 'radial',
 type = 'C-classification'
)
# 进行实例调优
instance <-  ti(
 task = tsk_sonar,
 learner = learner,
 resampling = rsmp('cv', folds = 3),
 measures = msr('classif.ce'),
 terminator = trm('none')
)
```

- 创建调优器

```r
tuner <- tnr('grid_search', resolution = 5, batch_size = 10)
```

- 接下来使用`Tuner$optimize(TuningInstanceBatchSingleCrit)`进行调参
  - 结果返回了最佳的调优参数，并且储存在了  
   `TuningInstanceBatchSingleCrit$result`中

```r
tuner$optimize(instance)
```

<table class='dataframe'>
<caption>A data.table: 1 x 5</caption>
<thead>
 <tr><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>learner_param_vals</th><th scope=col>x_domain</th><th scope=col>classif.ce</th></tr>
 <tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>25000.08</td><td>0.1</td><td>radial          , C-classification, 25000.075       , 0.1             </td><td>25000.08, 0.10</td><td>0.27902</td></tr>
</tbody>
</table>

```r
instance$result
instance$result$learner_param_vals
```

<table class='dataframe'>
<caption>A data.table: 1 x 5</caption>
<thead>
 <tr><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>learner_param_vals</th><th scope=col>x_domain</th><th scope=col>classif.ce</th></tr>
 <tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>25000.08</td><td>0.1</td><td>radial          , C-classification, 25000.075       , 0.1             </td><td>25000.08, 0.10</td><td>0.27902</td></tr>
</tbody>
</table>

```css
$kernel
'radial'
$type
'C-classification'
$cost
25000.075
$gamma
0.1
```

### 对数变换

- 对于许多具有较大上界的非负超参数，  
  在对数尺度上进行调优可能比在线性尺度上更有效
- 对数变化之后可以让备选参数的范围更集中

```r
# library(patchwork)
data.frame(x=cost) %>% ggplot(aes(x=x)) + geom_histogram() + theme_bw() & data.frame(x=exp_cost) %>% ggplot(aes(x=x)) + geom_histogram() + theme_bw()
```

![image_8](./image_8.png)

- 在调优时，只用将`logscale = TRUE` 传递给 `to_tune()` 即可

```r
# 调取任务
tsk_sonar <-  tsk('sonar')
# 构建学习器
learner <-  lrn('classif.svm',
 # 指定调优的超参`cose`和`gamma`
 # 选择将对数变换的结果传递给`Learner`
 cost  = to_tune(1e-1, 1e5, logscale = TRUE),
 gamma = to_tune(1e-1, 1e5, logscale = TRUE),
 # 设置其他的超参
 kernel = 'radial',
 type = 'C-classification'
)
# 进行实例调优
instance <-  ti(
 task = tsk_sonar,
 learner = learner,
 resampling = rsmp('cv', folds = 3),
 measures = msr('classif.ce'),
 terminator = trm('none')
)
# 构建调优器
tuner <- tnr('grid_search', resolution = 5, batch_size = 10)
# 调优实例
tuner$optimize(instance)
```

<table class='dataframe'>
<caption>A data.table: 1 x 5</caption>
<thead>
 <tr><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>learner_param_vals</th><th scope=col>x_domain</th><th scope=col>classif.ce</th></tr>
 <tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>1.151293</td><td>-2.302585</td><td>radial          , C-classification, 3.16227766016838, 0.1             </td><td>3.162278, 0.100000</td><td>0.2408558</td></tr>
</tbody>
</table>

- `cost`和`gamma`是对数变换前的最优值，  
  而`learner_param_vals`和`x_domain`包含的是是对数变换后的

```r
instance$result_learner_param_vals
instance$result$learner_param_vals
```

```css
$kernel
'radial'
$type
'C-classification'
$cost
3.16227766016838
$gamma
0.1
```

```r
instance$result_x_domain
instance$result$x_domain
```

```css
$cost
3.16227766016838
$gamma
0.1
```

- 可以简单验算一下结果
  - 结果和上方的结果完全一致

```r
instance$result$cost
exp(instance$result$cost)
```

```css
1.15129254649702
3.16227766016838
```

### 分析和使用结果

- 评估完的结果储存在`Instance$archive`里面
- 一共有25组，这是因为设定的`Tuner`的`resulution`是5，  
  所以可以得到$5^{2}= 25$

```r
instance$archive
```

<table class='dataframe'>
<caption>A data.table: 25 x 10</caption>
<thead>
 <tr><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>classif.ce</th><th scope=col>runtime_learners</th><th scope=col>timestamp</th><th scope=col>warnings</th><th scope=col>errors</th><th scope=col>x_domain</th><th scope=col>batch_nr</th><th scope=col>resample_result</th></tr>
 <tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dttm&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>-2.302585</td><td>11.512925</td><td>0.4665286</td><td>0.039</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>1e-01, 1e+05</td><td>1</td><td>&lt;environment: 0x1734a6650&gt;</td></tr>
 <tr><td> 1.151293</td><td>-2.302585</td><td>0.2408558</td><td>0.046</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 0.100000</td><td>1</td><td>&lt;environment: 0x173467a08&gt;</td></tr>
 <tr><td> 1.151293</td><td> 1.151293</td><td>0.4665286</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 3.162278</td><td>1</td><td>&lt;environment: 0x173430338&gt;</td></tr>
 <tr><td> 1.151293</td><td> 8.059048</td><td>0.4665286</td><td>0.026</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 3162.277660</td><td>1</td><td>&lt;environment: 0x1733e7c38&gt;</td></tr>
 <tr><td> 1.151293</td><td>11.512925</td><td>0.4665286</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278e+00, 1.000000e+05</td><td>1</td><td>&lt;environment: 0x1733b1f48&gt;</td></tr>
 <tr><td> 4.605170</td><td>-2.302585</td><td>0.2408558</td><td>0.100</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>100.0, 0.1</td><td>1</td><td>&lt;environment: 0x173087ad0&gt;</td></tr>
 <tr><td> 8.059048</td><td>-2.302585</td><td>0.2408558</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.278, 0.100</td><td>1</td><td>&lt;environment: 0x17361d128&gt;</td></tr>
 <tr><td> 8.059048</td><td>11.512925</td><td>0.4665286</td><td>0.027</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.278, 100000.000</td><td>1</td><td>&lt;environment: 0x173397c08&gt;</td></tr>
 <tr><td>11.512925</td><td>11.512925</td><td>0.4665286</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>1e+05, 1e+05</td><td>1</td><td>&lt;environment: 0x173350e78&gt;</td></tr>
 <tr><td>-2.302585</td><td>-2.302585</td><td>0.4665286</td><td>0.029</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>0.1, 0.1</td><td>2</td><td>&lt;environment: 0x173313660&gt;</td></tr>
 <tr><td>-2.302585</td><td> 8.059048</td><td>0.4665286</td><td>0.026</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>0.100, 3162.278</td><td>2</td><td>&lt;environment: 0x1732e5170&gt;</td></tr>
 <tr><td> 1.151293</td><td> 4.605170</td><td>0.4665286</td><td>0.025</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 100.000000</td><td>2</td><td>&lt;environment: 0x1732b32b0&gt;</td></tr>
 <tr><td> 4.605170</td><td> 1.151293</td><td>0.4665286</td><td>0.038</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>100.000000, 3.162278</td><td>2</td><td>&lt;environment: 0x1732860c8&gt;</td></tr>
 <tr><td> 8.059048</td><td> 1.151293</td><td>0.4665286</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.277660, 3.162278</td><td>2</td><td>&lt;environment: 0x171620ab0&gt;</td></tr>
 <tr><td> 8.059048</td><td> 8.059048</td><td>0.4665286</td><td>0.031</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.278, 3162.278</td><td>2</td><td>&lt;environment: 0x1715e60b0&gt;</td></tr>
 <tr><td>11.512925</td><td> 1.151293</td><td>0.4665286</td><td>0.035</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>1.000000e+05, 3.162278e+00</td><td>2</td><td>&lt;environment: 0x173054958&gt;</td></tr>
 <tr><td>11.512925</td><td> 8.059048</td><td>0.4665286</td><td>0.029</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>100000.000, 3162.278</td><td>2</td><td>&lt;environment: 0x173214b00&gt;</td></tr>
 <tr><td>-2.302585</td><td> 1.151293</td><td>0.4665286</td><td>0.031</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>0.100000, 3.162278</td><td>3</td><td>&lt;environment: 0x1731baa20&gt;</td></tr>
 <tr><td>-2.302585</td><td> 4.605170</td><td>0.4665286</td><td>0.027</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>0.1, 100.0</td><td>3</td><td>&lt;environment: 0x173160e10&gt;</td></tr>
 <tr><td> 4.605170</td><td> 4.605170</td><td>0.4665286</td><td>0.026</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>100, 100</td><td>3</td><td>&lt;environment: 0x1730f8d98&gt;</td></tr>
 <tr><td> 4.605170</td><td> 8.059048</td><td>0.4665286</td><td>0.040</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>100.000, 3162.278</td><td>3</td><td>&lt;environment: 0x173064ba0&gt;</td></tr>
 <tr><td> 4.605170</td><td>11.512925</td><td>0.4665286</td><td>0.064</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>1e+02, 1e+05</td><td>3</td><td>&lt;environment: 0x13a1f7410&gt;</td></tr>
 <tr><td> 8.059048</td><td> 4.605170</td><td>0.4665286</td><td>0.033</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>3162.278, 100.000</td><td>3</td><td>&lt;environment: 0x176b02dd8&gt;</td></tr>
 <tr><td>11.512925</td><td>-2.302585</td><td>0.2408558</td><td>0.035</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>1e+05, 1e-01</td><td>3</td><td>&lt;environment: 0x176c46388&gt;</td></tr>
 <tr><td>11.512925</td><td> 4.605170</td><td>0.4665286</td><td>0.039</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>1e+05, 1e+02</td><td>3</td><td>&lt;environment: 0x176bf3e68&gt;</td></tr>
</tbody>
</table>

- 可以根据不同的性能指标对内部`ResampleResult`进行评分
  - zhe l这里是考察漏报率、误报率以及分类错误

```r
as.data.table(
 instance$archive,
 measures = msrs(c('classif.fpr', 'classif.fnr'))
)
```

<table class='dataframe'>
<caption>A data.table: 25 x 12</caption>
<thead>
 <tr><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>classif.ce</th><th scope=col>classif.fpr</th><th scope=col>classif.fnr</th><th scope=col>runtime_learners</th><th scope=col>timestamp</th><th scope=col>warnings</th><th scope=col>errors</th><th scope=col>x_domain</th><th scope=col>batch_nr</th><th scope=col>resample_result</th></tr>
 <tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dttm&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th></tr>
</thead>
<tbody>
 <tr><td>-2.302585</td><td>11.512925</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.039</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>1e-01, 1e+05</td><td>1</td><td>&lt;environment: 0x17b2d4438&gt;</td></tr>
 <tr><td> 1.151293</td><td>-2.302585</td><td>0.2408558</td><td>0.4606607</td><td>0.03397436</td><td>0.046</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 0.100000</td><td>1</td><td>&lt;environment: 0x17b2bce48&gt;</td></tr>
 <tr><td> 1.151293</td><td> 1.151293</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 3.162278</td><td>1</td><td>&lt;environment: 0x17b2a6070&gt;</td></tr>
 <tr><td> 1.151293</td><td> 8.059048</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.026</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 3162.277660</td><td>1</td><td>&lt;environment: 0x17b288a58&gt;</td></tr>
 <tr><td> 1.151293</td><td>11.512925</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278e+00, 1.000000e+05</td><td>1</td><td>&lt;environment: 0x17b2719e0&gt;</td></tr>
 <tr><td> 4.605170</td><td>-2.302585</td><td>0.2408558</td><td>0.4606607</td><td>0.03397436</td><td>0.100</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>100.0, 0.1</td><td>1</td><td>&lt;environment: 0x17b254208&gt;</td></tr>
 <tr><td> 8.059048</td><td>-2.302585</td><td>0.2408558</td><td>0.4606607</td><td>0.03397436</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.278, 0.100</td><td>1</td><td>&lt;environment: 0x17b23cb00&gt;</td></tr>
 <tr><td> 8.059048</td><td>11.512925</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.027</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.278, 100000.000</td><td>1</td><td>&lt;environment: 0x17b19f510&gt;</td></tr>
 <tr><td>11.512925</td><td>11.512925</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>1e+05, 1e+05</td><td>1</td><td>&lt;environment: 0x17b17e0e0&gt;</td></tr>
 <tr><td>-2.302585</td><td>-2.302585</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.029</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>0.1, 0.1</td><td>2</td><td>&lt;environment: 0x17b1639e0&gt;</td></tr>
 <tr><td>-2.302585</td><td> 8.059048</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.026</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>0.100, 3162.278</td><td>2</td><td>&lt;environment: 0x17b140828&gt;</td></tr>
 <tr><td> 1.151293</td><td> 4.605170</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.025</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3.162278, 100.000000</td><td>2</td><td>&lt;environment: 0x17b1184e0&gt;</td></tr>
 <tr><td> 4.605170</td><td> 1.151293</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.038</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>100.000000, 3.162278</td><td>2</td><td>&lt;environment: 0x17b0ffee8&gt;</td></tr>
 <tr><td> 8.059048</td><td> 1.151293</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.030</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.277660, 3.162278</td><td>2</td><td>&lt;environment: 0x17b0e54d8&gt;</td></tr>
 <tr><td> 8.059048</td><td> 8.059048</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.031</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>3162.278, 3162.278</td><td>2</td><td>&lt;environment: 0x17b0c9120&gt;</td></tr>
 <tr><td>11.512925</td><td> 1.151293</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.035</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>1.000000e+05, 3.162278e+00</td><td>2</td><td>&lt;environment: 0x17b0ae860&gt;</td></tr>
 <tr><td>11.512925</td><td> 8.059048</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.029</td><td>2025-06-17 14:29:56</td><td>0</td><td>0</td><td>100000.000, 3162.278</td><td>2</td><td>&lt;environment: 0x17b0978c8&gt;</td></tr>
 <tr><td>-2.302585</td><td> 1.151293</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.031</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>0.100000, 3.162278</td><td>3</td><td>&lt;environment: 0x17b07ce80&gt;</td></tr>
 <tr><td>-2.302585</td><td> 4.605170</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.027</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>0.1, 100.0</td><td>3</td><td>&lt;environment: 0x17b062f60&gt;</td></tr>
 <tr><td> 4.605170</td><td> 4.605170</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.026</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>100, 100</td><td>3</td><td>&lt;environment: 0x17b049820&gt;</td></tr>
 <tr><td> 4.605170</td><td> 8.059048</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.040</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>100.000, 3162.278</td><td>3</td><td>&lt;environment: 0x17b02c320&gt;</td></tr>
 <tr><td> 4.605170</td><td>11.512925</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.064</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>1e+02, 1e+05</td><td>3</td><td>&lt;environment: 0x17b015b30&gt;</td></tr>
 <tr><td> 8.059048</td><td> 4.605170</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.033</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>3162.278, 100.000</td><td>3</td><td>&lt;environment: 0x1797f0da0&gt;</td></tr>
 <tr><td>11.512925</td><td>-2.302585</td><td>0.2408558</td><td>0.4606607</td><td>0.03397436</td><td>0.035</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>1e+05, 1e-01</td><td>3</td><td>&lt;environment: 0x1797d1698&gt;</td></tr>
 <tr><td>11.512925</td><td> 4.605170</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td><td>0.039</td><td>2025-06-17 14:29:57</td><td>0</td><td>0</td><td>1e+05, 1e+02</td><td>3</td><td>&lt;environment: 0x1797b2a90&gt;</td></tr>
</tbody>
</table>

- 只挑前五个的一部分

```r
as.data.table(
 instance$archive,
 measures = msrs(c('classif.fpr', 'classif.fnr'))
)[1:5 ,.(cost, gamma, classif.ce, classif.fpr, classif.fnr)]
```

<table class='dataframe'>
<caption>A data.table: 5 x 5</caption>
<thead>
 <tr><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>classif.ce</th><th scope=col>classif.fpr</th><th scope=col>classif.fnr</th></tr>
 <tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>-2.302585</td><td>11.512925</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td></tr>
 <tr><td> 1.151293</td><td>-2.302585</td><td>0.2408558</td><td>0.4606607</td><td>0.03397436</td></tr>
 <tr><td> 1.151293</td><td> 1.151293</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td></tr>
 <tr><td> 1.151293</td><td> 8.059048</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td></tr>
 <tr><td> 1.151293</td><td>11.512925</td><td>0.4665286</td><td>1.0000000</td><td>0.00000000</td></tr>
</tbody>
</table>

- `Instance$archive$benchmark_result`中储存了所有的`BenchmarkResult`对象

```r
{instance$archive$benchmark_result %>% as.data.table()}[1:5]
```

<table class="dataframe">
<caption>A data.table: 5 x 9</caption>
<thead>
 <tr><th scope=col>uhash</th><th scope=col>task</th><th scope=col>learner</th><th scope=col>resampling</th><th scope=col>iteration</th><th scope=col>prediction</th><th scope=col>task_id</th><th scope=col>learner_id</th><th scope=col>resampling_id</th></tr>
 <tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
 <tr><td>e31b69c6-41d4-4d7f-81c2-ed3b4bd3b75f</td><td>&lt;environment: 0x17cd32908&gt;</td><td>&lt;environment: 0x3064c38c0&gt;</td><td>&lt;environment: 0x17269e738&gt;</td><td>1</td><td>&lt;environment: 0x3202d2ac0&gt;</td><td>sonar</td><td>classif.svm</td><td>cv</td></tr>
 <tr><td>e31b69c6-41d4-4d7f-81c2-ed3b4bd3b75f</td><td>&lt;environment: 0x17cd32908&gt;</td><td>&lt;environment: 0x306425260&gt;</td><td>&lt;environment: 0x17269e738&gt;</td><td>2</td><td>&lt;environment: 0x32023dcf0&gt;</td><td>sonar</td><td>classif.svm</td><td>cv</td></tr>
 <tr><td>e31b69c6-41d4-4d7f-81c2-ed3b4bd3b75f</td><td>&lt;environment: 0x17cd32908&gt;</td><td>&lt;environment: 0x30a0e4660&gt;</td><td>&lt;environment: 0x17269e738&gt;</td><td>3</td><td>&lt;environment: 0x32019c9d0&gt;</td><td>sonar</td><td>classif.svm</td><td>cv</td></tr>
 <tr><td>5413c20f-f67c-4741-bc1e-6535ac83e266</td><td>&lt;environment: 0x17cd32908&gt;</td><td>&lt;environment: 0x300847120&gt;</td><td>&lt;environment: 0x17269e738&gt;</td><td>1</td><td>&lt;environment: 0x320379c40&gt;</td><td>sonar</td><td>classif.svm</td><td>cv</td></tr>
 <tr><td>5413c20f-f67c-4741-bc1e-6535ac83e266</td><td>&lt;environment: 0x17cd32908&gt;</td><td>&lt;environment: 0x31f42b8c0&gt;</td><td>&lt;environment: 0x17269e738&gt;</td><td>2</td><td>&lt;environment: 0x17ede9278&gt;</td><td>sonar</td><td>classif.svm</td><td>cv</td></tr>
</tbody>
</table>

- 可以将`type = "surface"`传递给`autoplot.TuningInstanceBatchSingleCrit`来可视化实例调优的结果
  - 蓝色的区域表示`classif.ce`评估器的较小结果，表明模型更优

```r
autoplot(instance, type = "surface")
```

![image_9](./image_9.png)

### 训练优化模型

- 当确定好超参后，将`Instance`中的超参传递给新建学习器中

```r
lrn_svm_tuned <- lrn("classif.svm")
lrn_svm_tuned$param_set$values <- instance$result_learner_param_vals
```

- 之后进行最基础的训练流程即可

```r
lrn_svm_tuned$train(tsk_sonar)$model
```

```r
Call:
svm.default(x = data, y = task$truth(), type = "C-classification", 
    kernel = "radial", gamma = 0.1, cost = 3.16227766016838, probability = (self$predict_type == 
        "prob"))


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  3.162278 

Number of Support Vectors:  205
```

## 更便捷的调优方法

- 基本调参方法中用的是`ti()`创建调优实例，  
  之后用`Tuner$optimize()`进行调优

### `tune()`

- `tune()`可以在创建实例后直接进行调优，  
  省略手动先创建调优实例，后
- 下面的例子可以进行简单的对比

```r
# 调取任务
tsk_sonar <-  tsk('sonar')

# ---------------之前的方法
# 构建学习器
learner <-  lrn('classif.svm',
 # 指定调优的超参`cose`和`gamma`
 # 选择将对数变换的结果传递给`Learner`
 cost  = to_tune(1e-1, 1e5, logscale = TRUE),
 gamma = to_tune(1e-1, 1e5, logscale = TRUE),
 # 设置其他的超参
 kernel = 'radial',
 type = 'C-classification'
)
# 进行实例调优
instance <-  ti(
 task = tsk_sonar,
 learner = learner,
 resampling = rsmp('cv', folds = 3),
 measures = msr('classif.ce'),
 terminator = trm('none')
)
# 构建调优器
tuner <- tnr('grid_search', resolution = 5, batch_size = 10)
# 调优实例
tuner$optimize(instance)

# ---------------`tune()`的方法
# 构建学习器
lrn_svm <- lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)
# [构建调优器]
# 需要先构建调优器，之后传递到`tune()`中
tnr_grid_search <- tnr("grid_search", resolution = 5, batch_size = 5)
# 进行实例调优
instance <- tune(
 # [这里多了`tuner`参数]
 # 所以先构建的调优器，后传递到`tune()`中
 # 当然也可以直接输入`tnr("grid_search", resolution = 5, batch_size = 5)`
 tuner = tnr_grid_search,
 task = tsk_sonar,
 learner = lrn_svm,
 resampling = rsmp("cv", folds = 3),
 measures = msr("classif.ce")
)
instance$result
```

### `auto_tune()`

- `auto_tune()` 创建一个`AutoTuner`类的对象
- `AutoTuner`继承自Learner类，并封装了调优所需的所有信息，  
  这意味着`AutoTuner`就是一个`Learner`
- 在底层，`AutoTuner`本质上是在调用`Learner$train()`时传递给模型的数据上运行`tune()`，然后将`Learner$param_set`参数设置为最优配置

```r
# 调取任务
tsk_sonar <-  tsk('sonar')

# ---------------之前的方法
# 构建学习器
learner <-  lrn('classif.svm',
 # 指定调优的超参`cose`和`gamma`
 # 选择将对数变换的结果传递给`Learner`
 cost  = to_tune(1e-1, 1e5, logscale = TRUE),
 gamma = to_tune(1e-1, 1e5, logscale = TRUE),
 # 设置其他的超参
 kernel = 'radial',
 type = 'C-classification'
)
# 进行实例调优
instance <-  ti(
 task = tsk_sonar,
 learner = learner,
 resampling = rsmp('cv', folds = 3),
 measures = msr('classif.ce'),
 terminator = trm('none')
)
# 构建调优器
tuner <- tnr('grid_search', resolution = 5, batch_size = 10)
# 调优实例
tuner$optimize(instance)

# ---------------`auto_tune()`的方法
# 构建学习器
lrn_svm <- lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification3333"
)
# [构建调优器]
# 需要先构建调优器，之后传递到`auto_tune()`中
tnr_grid_search <- tnr("grid_search", resolution = 5, batch_size = 5)
# 创建`AutoTuner`
at <- auto_tuner(
  tuner = tnr_grid_search,
  learner = lrn_svm,
  resampling = rsmp('cv', folds = 3),
  measure = msr('classif.ce')
)
# 分割`Task`的数据
split <- partition(tsk_sonar) 
# 训练模型，在此过程中自动调优选择出最佳的超参
at$train(tsk_sonar, row_ids = split$train) 
# 预测模型，获得评估结果
pred <- at$predict(tsk_sonar, row_ids = split$test)
pred$score()
```

```css
classif.ce: 0.159420289855072
```

- `AutoTuner`中还包含一个调优实例`Intance`
  - 用法和前述一样

```r
at$tuning_instance$result %>% as.data.table()
```

<table class="dataframe">
<caption>A data.table: 1 x 5</caption>
<thead>
 <tr><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>learner_param_vals</th><th scope=col>x_domain</th><th scope=col>classif.ce</th></tr>
 <tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
 <tr><td>11.51293</td><td>-5.756463</td><td>radial             , C-classification   , 1e+05              , 0.00316227766016838</td><td>1.000000e+05, 3.162278e-03</td><td>0.2008942</td></tr>
</tbody>
</table>

![image_10](./image_10.png)
> Illustration of an Auto-Tuner.

## 嵌套重采样（Nested Resampling）

### 简介

- 超参数优化在不进行额外的重采样时会产生训练误差导致的乐观偏差

![image_11](./image_11.png)

嵌套重采样示意图。大块代表用于模型评估的外部重采样的三折交叉验证，小块代表用于调参的内部重采样的四折交叉验证。浅蓝色块为训练集，深蓝色块为测试集。

- 看着挺挺复杂的，在本例中：
  - 其实就是先对整个数据集进行3折交叉验证的切分，  
   这个里面2/3的用于训练集，1/3的用于验证集  
   （本质是测试集，不过一般测试集是纯用于测试的那种），
   这个部分叫**外部重采样（outer resampling）**
    - 对2/3训练集中再进行4折交叉验证的切分，  
    这个部分叫**内部重采样（inner resampling）**
    - 内部重采样训练出来的N个模型用外部的验证集进行评估，  
    选出最佳超参，  
    这个部分叫**超参数优化（HPO）**
      - N取决于超参个数以及网格搜索分辨率，  
     2个超参，网格搜索分辨率为2就是$2^2=4$，  
     2个超参，网格搜索分辨率为3就是$3^2=9$，  
     这个结果再乘以内部重采样的k值
      - 本例中假设内部重采样是4折，2个参数，2个网格，  
     那么共产生 $4\times2^2=16$ 个模型
      - 这些模型不直接用于1/3数据的验证集，  
     而是用内部重采样的小验证集进行性能评估，  
     最终选出16个模型里在自己小验证集上表现最好的模型的超参
    - 用选择的超参在**整个**2/3数据的的训练集进行模型训练，  
    再在1/3数据的测试集上进行模型评估
  - 上述一轮的内外重采样结束后，再重复两轮
  - 三次结束后，获得最终3个模型，  
   最这三个模型进行综合评估并选择最佳参数建模，  
   并最终用测试集评估最终性能

- **注意**：嵌套重采样的计算成本很高
  - 正如上所述，超参的个数和网格数分辨率决定了训练模型的迭代次数；  
   如果2超参外3折内4折为5的分辨率，那么总共需要迭代 $3\times4\times5^{2}= 300$次
  - 如果计算资源支持，可以通过并行化提高速度

### 使用`auto_tuner()`进行嵌套重采样

- 只需将 `AutoTuner` 传递给 `resample()`或 `benchmark()` 即可

```r
# 调取任务
tsk_sonar <-  tsk('sonar')
# 构建学习器
lrn_svm <- lrn('classif.svm',
 cost  = to_tune(1e-5, 1e5, logscale = TRUE),
 gamma = to_tune(1e-5, 1e5, logscale = TRUE),
 kernel = 'radial',
 type = 'C-classification'
)
# 构建调优器
tnr_grid_search <- tnr('grid_search', resolution = 5, batch_size = 5)
# 构建`AutoTuner`
at <- auto_tuner(
 tuner = tnr_grid_search,
 learner = lrn_svm,
 # 构建内部`Resampling`
 resampling = rsmp('cv', folds = 4),
 measure = msr('classif.ce')
)
# 构建外部`Resampling`
rsmp_cv3 <- rsmp('cv', folds = 3)
# 将`AutoTuner`传递给`resample()`
# 这里的`store_models=TRUE`仅能储存外部重采样的3个模型
rr <- resample(tsk_sonar, at, rsmp_cv3, store_models = TRUE)
# 打印
rr
```

```css
[36m--[39m [1m[34m<ResampleResult>[39m with [34m3[39m resampling iterations[22m [36m-------------------------------[39m
 task_id        learner_id resampling_id iteration     prediction_test warnings
   sonar classif.svm.tuned            cv         1 <PredictionClassif>        0
   sonar classif.svm.tuned            cv         2 <PredictionClassif>        0
   sonar classif.svm.tuned            cv         3 <PredictionClassif>        0
 errors
      0
      0
      0
```

- 查看最终模型的

```r
rr$aggregate()
```

```css
classif.ce: 0.163285024154589
```

- `extract_inner_tuning_results(ResampleResult)` 返回外部重采样的全部结果  
- `extract_inner_tuning_archives(ResampleResult)` 返回内外部重采样的全部结果

```r
extract_inner_tuning_results(rr)
```

<table class='dataframe'>
<caption>A data.table: 3 x 9</caption>
<thead>
 <tr><th scope=col>iteration</th><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>classif.ce</th><th scope=col>learner_param_vals</th><th scope=col>x_domain</th><th scope=col>task_id</th><th scope=col>learner_id</th><th scope=col>resampling_id</th></tr>
 <tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
 <tr><td>1</td><td>5.756463</td><td> -5.756463</td><td>0.1882353</td><td>radial             , C-classification   , 316.227766016838   , 0.00316227766016838</td><td>3.162278e+02, 3.162278e-03</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td>5.756463</td><td>-11.512925</td><td>0.2376050</td><td>radial          , C-classification, 316.227766016838, 1e-05           </td><td>316.22777, 0.00001</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td>5.756463</td><td> -5.756463</td><td>0.2006303</td><td>radial             , C-classification   , 316.227766016838   , 0.00316227766016838</td><td>3.162278e+02, 3.162278e-03</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
</tbody>
</table>

```r
extract_inner_tuning_archives(rr)
```

<table class='dataframe'>
<caption>A data.table: 75 x 15</caption>
<thead>
 <tr><th scope=col>iteration</th><th scope=col>cost</th><th scope=col>gamma</th><th scope=col>classif.ce</th><th scope=col>x_domain_cost</th><th scope=col>x_domain_gamma</th><th scope=col>runtime_learners</th><th scope=col>timestamp</th><th scope=col>warnings</th><th scope=col>errors</th><th scope=col>batch_nr</th><th scope=col>resample_result</th><th scope=col>task_id</th><th scope=col>learner_id</th><th scope=col>resampling_id</th></tr>
 <tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dttm&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;list&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
 <tr><td>1</td><td>-11.512925</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4632353</td><td>1.000000e-05</td><td>3.162278e+02</td><td>0.088</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16ec87bc8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td> -5.756463</td><td>0.2689076</td><td>1.000000e+00</td><td>3.162278e-03</td><td>0.042</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16ec63f78&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4632353</td><td>1.000000e+00</td><td>1.000000e+00</td><td>0.074</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16ec47d10&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>-11.512925</td><td>0.3126050</td><td>3.162278e+02</td><td>1.000000e-05</td><td>0.033</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16ec1b1c8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4632353</td><td>3.162278e+02</td><td>1.000000e+00</td><td>0.035</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16ebf3920&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> -5.756463</td><td> -5.756463</td><td>0.4632353</td><td>3.162278e-03</td><td>3.162278e-03</td><td>0.026</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x16ebda1e0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> -5.756463</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4632353</td><td>3.162278e-03</td><td>1.000000e+00</td><td>0.029</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x16eba3970&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> -5.756463</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4632353</td><td>3.162278e-03</td><td>3.162278e+02</td><td>0.037</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x16eb76288&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td> 11.512925</td><td>0.4632353</td><td>1.000000e+00</td><td>1.000000e+05</td><td>0.028</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x16eb1ca18&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> 11.512925</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4632353</td><td>1.000000e+05</td><td>3.162278e+02</td><td>0.034</td><td>2025-06-17 23:57:50</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x16eaf0358&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td>-11.512925</td><td> -5.756463</td><td>0.4632353</td><td>1.000000e-05</td><td>3.162278e-03</td><td>0.035</td><td>2025-06-17 23:57:51</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x16eac7780&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td>-11.512925</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4632353</td><td>1.000000e-05</td><td>1.000000e+00</td><td>0.029</td><td>2025-06-17 23:57:51</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x16e7cc3a0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td>-11.512925</td><td> 11.512925</td><td>0.4632353</td><td>1.000000e-05</td><td>1.000000e+05</td><td>0.055</td><td>2025-06-17 23:57:51</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x134617800&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4632353</td><td>1.000000e+00</td><td>3.162278e+02</td><td>0.044</td><td>2025-06-17 23:57:51</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x164f9f808&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> 11.512925</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4632353</td><td>1.000000e+05</td><td>1.000000e+00</td><td>0.039</td><td>2025-06-17 23:57:51</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x164f80080&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> -5.756463</td><td> 11.512925</td><td>0.4632353</td><td>3.162278e-03</td><td>1.000000e+05</td><td>0.029</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x164f5c5f8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td> -5.756463</td><td>0.1882353</td><td>3.162278e+02</td><td>3.162278e-03</td><td>0.025</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x164f362f0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td> 11.512925</td><td>0.4632353</td><td>3.162278e+02</td><td>1.000000e+05</td><td>0.050</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x164f0c570&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> 11.512925</td><td> -5.756463</td><td>0.1882353</td><td>1.000000e+05</td><td>3.162278e-03</td><td>0.024</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x164eec7c8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> 11.512925</td><td> 11.512925</td><td>0.4632353</td><td>1.000000e+05</td><td>1.000000e+05</td><td>0.792</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x164ec0b48&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td>-11.512925</td><td>-11.512925</td><td>0.4632353</td><td>1.000000e-05</td><td>1.000000e-05</td><td>0.030</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x164e8ab88&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> -5.756463</td><td>-11.512925</td><td>0.4632353</td><td>3.162278e-03</td><td>1.000000e-05</td><td>0.029</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x164e5e2d0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>-11.512925</td><td>0.4632353</td><td>1.000000e+00</td><td>1.000000e-05</td><td>0.043</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x164e34400&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4632353</td><td>3.162278e+02</td><td>3.162278e+02</td><td>0.074</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x164e02020&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>1</td><td> 11.512925</td><td>-11.512925</td><td>0.2542017</td><td>1.000000e+05</td><td>1.000000e-05</td><td>0.026</td><td>2025-06-17 23:57:52</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x164de4160&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td> -5.756463</td><td>-11.512925</td><td>0.4672269</td><td>3.162278e-03</td><td>1.000000e-05</td><td>0.030</td><td>2025-06-17 23:57:53</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16e41f860&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4672269</td><td>1.000000e+00</td><td>3.162278e+02</td><td>0.050</td><td>2025-06-17 23:57:53</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16e3e9f40&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4672269</td><td>3.162278e+02</td><td>3.162278e+02</td><td>0.035</td><td>2025-06-17 23:57:53</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16e3c77f8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td> 11.512925</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4672269</td><td>1.000000e+05</td><td>3.162278e+02</td><td>0.032</td><td>2025-06-17 23:57:53</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x1345eef68&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td> 11.512925</td><td> 11.512925</td><td>0.4672269</td><td>1.000000e+05</td><td>1.000000e+05</td><td>0.029</td><td>2025-06-17 23:57:53</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x1345c8a18&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>
 <tr><td>2</td><td>-11.512925</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4672269</td><td>1.000000e-05</td><td>3.162278e+02</td><td>0.027</td><td>2025-06-17 23:57:54</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x164ca6460&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td> -5.756463</td><td> -5.756463</td><td>0.4672269</td><td>3.162278e-03</td><td>3.162278e-03</td><td>0.029</td><td>2025-06-17 23:57:54</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x164c86108&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td> -5.756463</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4672269</td><td>3.162278e-03</td><td>3.162278e+02</td><td>0.023</td><td>2025-06-17 23:57:54</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x13452b258&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4672269</td><td>1.000000e+00</td><td>1.000000e+00</td><td>0.026</td><td>2025-06-17 23:57:54</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x1345010e8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>2</td><td> 11.512925</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4672269</td><td>1.000000e+05</td><td>1.000000e+00</td><td>0.028</td><td>2025-06-17 23:57:54</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x1344dd2a8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td>-11.512925</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4680672</td><td>1.000000e-05</td><td>1.000000e+00</td><td>0.033</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16e8c96d8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> -5.756463</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4680672</td><td>3.162278e-03</td><td>1.000000e+00</td><td>0.042</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16e8a5b28&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td> -5.756463</td><td>0.2518908</td><td>1.000000e+00</td><td>3.162278e-03</td><td>0.025</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x16e87db78&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4680672</td><td>3.162278e+02</td><td>1.000000e+00</td><td>0.026</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x164b0ec00&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> 11.512925</td><td>-11.512925</td><td>0.2371849</td><td>1.000000e+05</td><td>1.000000e-05</td><td>0.051</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>1</td><td>&lt;environment: 0x164aede90&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td>-11.512925</td><td> -5.756463</td><td>0.4680672</td><td>1.000000e-05</td><td>3.162278e-03</td><td>0.027</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x164aadb78&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td>-11.512925</td><td> 11.512925</td><td>0.4680672</td><td>1.000000e-05</td><td>1.000000e+05</td><td>0.026</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x164a8cee8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> -5.756463</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4680672</td><td>3.162278e-03</td><td>3.162278e+02</td><td>0.024</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x164a46ef8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td> -5.756463</td><td>0.2006303</td><td>3.162278e+02</td><td>3.162278e-03</td><td>0.024</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x164a275b0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4680672</td><td>3.162278e+02</td><td>3.162278e+02</td><td>0.025</td><td>2025-06-17 23:57:55</td><td>0</td><td>0</td><td>2</td><td>&lt;environment: 0x164a0a2e0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> -5.756463</td><td>-11.512925</td><td>0.4823529</td><td>3.162278e-03</td><td>1.000000e-05</td><td>0.025</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x1649ea238&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> -5.756463</td><td> -5.756463</td><td>0.4680672</td><td>3.162278e-03</td><td>3.162278e-03</td><td>0.030</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x1649d0c80&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> -5.756463</td><td> 11.512925</td><td>0.4680672</td><td>3.162278e-03</td><td>1.000000e+05</td><td>0.052</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x1649b6c10&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>-11.512925</td><td>0.4823529</td><td>1.000000e+00</td><td>1.000000e-05</td><td>0.029</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x16e865190&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td> 11.512925</td><td>0.4680672</td><td>1.000000e+00</td><td>1.000000e+05</td><td>0.028</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>3</td><td>&lt;environment: 0x16e84baf8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td>-11.512925</td><td>-11.512925</td><td>0.4823529</td><td>1.000000e-05</td><td>1.000000e-05</td><td>0.037</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x16e82a278&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td>-11.512925</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4680672</td><td>1.000000e-05</td><td>3.162278e+02</td><td>0.025</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x133ff9c88&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>-11.512925</td><td>0.2802521</td><td>3.162278e+02</td><td>1.000000e-05</td><td>0.025</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x133fde710&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td> 11.512925</td><td>0.4680672</td><td>3.162278e+02</td><td>1.000000e+05</td><td>0.026</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x133fc2fd0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> 11.512925</td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4680672</td><td>1.000000e+05</td><td>3.162278e+02</td><td>0.027</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>4</td><td>&lt;environment: 0x133f06cb0&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4680672</td><td>1.000000e+00</td><td>1.000000e+00</td><td>0.025</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x133eca480&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td><span style=white-space:pre-wrap>  5.756463</span></td><td>0.4680672</td><td>1.000000e+00</td><td>3.162278e+02</td><td>0.024</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x133e9dd28&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> 11.512925</td><td> -5.756463</td><td>0.2006303</td><td>1.000000e+05</td><td>3.162278e-03</td><td>0.033</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x133e7f238&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> 11.512925</td><td><span style=white-space:pre-wrap>  0.000000</span></td><td>0.4680672</td><td>1.000000e+05</td><td>1.000000e+00</td><td>0.029</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x16e33a6d8&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
 <tr><td>3</td><td> 11.512925</td><td> 11.512925</td><td>0.4680672</td><td>1.000000e+05</td><td>1.000000e+05</td><td>0.027</td><td>2025-06-17 23:57:56</td><td>0</td><td>0</td><td>5</td><td>&lt;environment: 0x16e319e18&gt;</td><td>sonar</td><td>classif.svm.tuned</td><td>cv</td></tr>
</tbody>
</table>

### 评估性能的正确（与错误）方式

- 这里进行一个对比，看看为什么要用嵌套重采样而不是简单的无嵌套重采样

```r
# ---------------准备数据
# 设置随机数种子
set.seed(5)
# 构建学习器
lrn_xgboost <- lrn(
 # 选择xgboost算法作为学习器
 'classif.xgboost',
 # 设置超参的选择范围
 eta = to_tune(1e-4, 1, logscale = TRUE),
 max_depth = to_tune(1, 20),
 colsample_bytree  = to_tune(1e-1, 1),
 colsample_bylevel = to_tune(1e-1, 1),
 lambda = to_tune(1e-3, 1e3, logscale = TRUE),
 alpha = to_tune(1e-3, 1e3, logscale = TRUE),
 subsample = to_tune(1e-1, 1)
)
# 构建任务生成器，命名为'moons'
tsk_moons <- tgen('moons')
# 随机生成100个训练样本和1000000个测试样本
tsk_moons_train <- tsk_moons$generate(100)
tsk_moons_test <- tsk_moons$generate(1000000)
# 构建评估器
msr_ce <- msr('classif.ce')
# 调优器用随机搜索法
tnr_random <- tnr('random_search')
# 使用留出法重采样
rsmp_holdout <- rsmp('holdout')
# 设置终止器，使用次数限制法，随机搜索700次终止
trm_evals700 <- trm('evals', n_evals = 700)

# -------无嵌套重采样
# 使用`tune()`进行实例调优
instance <- tune(
 tuner = tnr_random,
 task = tsk_moons_train,
 learner = lrn_xgboost,
 resampling = rsmp_holdout,
 measures = msr_ce,
 terminator = trm_evals700
)
# 记录最优参数在调优集上的性能
insample <- instance$result_y

# -------嵌套重采样
# 使用`auto_tuner()`创建一个调优学习器`AutoTuner`
at <- auto_tuner(
 tuner = tnr_random,
 learner = lrn_xgboost,
 # 这里的留出法用于内部重采样
 resampling = rsmp_holdout,
 measure = msr_ce,
 terminator = trm_evals700
)
# 创建一个5折CV，用于后续的外部重采样
rsmp_cv5 <- rsmp('cv', folds = 5)
# 将`Task`，`AutoTuner`和外部重采样器`Resampling`传递给`resample()`
# `ResampleResult$aggregate()`给出整合结果
rr <- resample(tsk_moons_train, at, rsmp_cv5)
outsample <- rr$aggregate()

# ------用无嵌套重采样的超参建模
# 创建xgboost的学习器
lrn_xgboost_tuned <- lrn('classif.xgboost')
# 将无嵌套重采样的实例调优得到的最佳超参传递给xgboost学习器
lrn_xgboost_tuned$param_set$set_values(
 # `instance`是无嵌套重采样的调优实例
 .values = instance$result_learner_param_vals
)
# 训练模型 -> 测试模型 -> 评估
generalization <- lrn_xgboost_tuned$train(
 tsk_moons_train
)$
 predict(tsk_moons_test)$
 score()

# 汇总一下结果
round(
 c(
  # 用无嵌套重采样选择的最佳超参构建的模型在测试集上的评估结果
  true_generalization = as.numeric(generalization),
  # 无嵌套重采样选择的最佳超参构建的模型在内部训练的评估结果
  # 这里的内部是仅在训练集上生成的调优模型得到的结果
  without_nested_resampling = as.numeric(insample),
  # 嵌套重采样选择的最佳超参构建的模型在内部训练的评估结果
  with_nested_resampling = as.numeric(outsample)
 ), 
 2
)
```

```css
true_generalization 0.1 
without_nested_resampling 0.06 
with_nested_resampling 0.14
```

- **从上面得到什么结果？**
  - 无嵌套重采样的内部评估得到的交叉熵很低，与实际在测试集的评估结果差异很大
  - 这可能表明对特定的内部留出划分存在**元过拟合（meta-overfitting）**
  - 相比之下嵌套重采样得到的外部估计效果更稳健

## 更高级的搜索空间

### 标量参数调优（Scalar Parameter Tuning）

- 当在学习器中使用`to_tune()`时，会隐式地为调优搜索空间创建一个`ParamSet`

```r
learner <- lrn('classif.svm',
  cost  = to_tune(1e-1, 1e5),
  kernel = to_tune(c('radial', 'linear')),
  shrinking = to_tune(),
  type = 'C-classification'
)

learner$param_set$search_space()
```

```css
<ParamSet(3)>
          id    class lower upper nlevels        default  value
      <char>   <char> <num> <num>   <num>         <list> <list>
1:      cost ParamDbl   0.1 1e+05     Inf <NoDefault[0]> [NULL]
2:    kernel ParamFct    NA    NA       2 <NoDefault[0]> [NULL]
3: shrinking ParamLgl    NA    NA       2           TRUE [NULL]
```

- 可以看到
  - `kernel`是因子，只需要传入想要调整的水平相对应的向量
  - `shrinking`超参数是一个逻辑值，它只有两种可能的值，  
   所以不需要向`to_tune()`传递任何内容，  
   它会自动从`learner$param_set`中识别出这是一个逻辑值，  
   并将这个细节传递给`learner$param_set$search_space()`
  - 对于因子参数，如果想在所有可能的值上进行调整，  
   也可以在不传递任何参数的情况下使用`to_tune()`
  - 如果想在一小部分可能的值上离散化数值参数，  
   可以使用to_tune()将数值参数视为因子
    - 例如，想找到随机森林中最优的树的数量，  
    我们可能只考虑三种情况：100棵、200棵或400棵树

```r
lrn('classif.ranger', num.trees = to_tune(c(100, 200, 400)))
```

- 将**整数变做因子**会导致超参数**无序**
  - 因此，利用顺序信息的算法在忽略顺序时性能会变差
  - 对于这些算法，  
   使用自定义变换定义一个 `ParamDbl` 或 `ParamInt`更有意义
   （后续内容）

### 定义搜索空间

- 下列`p_*()`创建`Param`对象

| Constructor 构造函数 | Description 描述                       | Underlying Class 基础类 |
| ---------------- | ------------------------------------ | -------------------- |
| `p_dbl()`        | Real valued parameter (“double”)     | `ParamDbl`           |
| `p_int()`        | Integer parameter                    | `ParamInt`           |
| `p_fct()`        | Discrete valued parameter (“factor”) | `ParamFct`           |
| `p_lgl()`        | Logical / Boolean parameter          | `ParamLgl`           |
| `p_uty()`        | Untyped parameter                    | `ParamUty`           |

- 使用`ps()`手动创造搜索空间，对象为`ParamSet`

```r
search_space <- ps(
 cost  = p_dbl(lower = 1e-1, upper = 1e5),
 kernel = p_fct(c('radial', 'linear')),
 shrinking = p_lgl()
)
```

- 将搜索空间传递给`ti()`
  - 官方文档写错了，他给的例子是`ti()`
   `auto_tuner()`应该传递了`Tuner`但是例子里没有`Tuner`

```r
tsk_sonar <- tsk('sonar')
rsmp_cv3 <- rsmp('cv', folds = 3)
msr_ce <- msr('classif.ce')

ists <- ti(
    tsk_sonar, 
    lrn('classif.svm', type = 'C-classification'), 
    rsmp_cv3,
    msr_ce, 
    trm('none'), 
    # 将搜索空间传递到参数`search_space`
    search_space = search_space
)

ists
```

```css
[36m--[39m [1m[34m<TuningInstanceBatchSingleCrit>[39m[22m [36m---------------------------------------------[39m
* State: Not optimized
* Objective: [34m<ObjectiveTuningBatch>[39m
* Search Space:
          id    class lower upper nlevels
      <char>   <char> <num> <num>   <num>
1:      cost ParamDbl   0.1 1e+05     Inf
2:    kernel ParamFct    NA    NA       2
3: shrinking ParamLgl    NA    NA       2
* Terminator: [34m<TerminatorNone>[39m
```

- **有界搜索空间**
  - 手动创建搜索空间时，对搜索空间中的所有超参是否有边界保持谨慎
  - 如果没有给`p_dbl()` 或 `p_int()` 传递下限和上限，  
   几乎所有调谐器在调优过程中都会抛出错误
  - 如果不确定，您可以在构造的 `ParamSet` （`ps()`创建的对象）上  
   使用`ParamSet$is_bounded`查看是否设置了下限和上限

```r
ps(cost = p_dbl(lower = 0.1, upper = 1))$is_bounded
ps(cost = p_dbl(lower = 0.1, upper = Inf))$is_bounded
search_space
```

```css
cost: TRUE

cost: FALSE

cost TRUE 
kernel TRUE 
shrinking TRUE
```

### 向量上的变换与调优

#### 使用`p_*()`的自定义变换

- 之前讨论过对数变换的用法
- 简单回顾一下之前用给`lrn()`传递用`to_tune()`创建的学习器的搜索空间

```r
svm <- lrn(
 'classif.svm',
 cost = to_tune(1e-5,1e5, logscale = TRUE)
)
svm$param_set
```

```css
<ParamSet(1)>
       id    class     lower    upper nlevels        default  value
   <char>   <char>     <num>    <num>   <num>         <list> <list>
1:   cost ParamDbl -11.51293 11.51293     Inf <NoDefault[0]> [NULL]
Trafo is set.
```

- 可以看到这里的`lower`和`upper`并非`1e-5`和`1e5`而是`-11.5`和`11.5` 　
  所以输入到学习器的范围其实是`log(1e-5)`和`log(1e5)`

- 手动创建对数变换
  - `trafo`中传递的是对采样结果的操作函数
  - 这里的输出结果和`svm$param_set`一致

```r
ss <- ps(
 cost = p_dbl(
  # 注意：这里是log(1e-5)和log(1e5)
  log(1e-5), log(1e5),
  # 对结果做指数变换
  trafo = function(x) ruturn(exp(x))
 )
) 
ss
```

```css
<ParamSet(1)>
       id    class     lower    upper nlevels        default  value
   <char>   <char>     <num>    <num>   <num>         <list> <list>
1:   cost ParamDbl -11.51293 11.51293     Inf <NoDefault[0]> [NULL]
Trafo is set.
```

- 虽然`ParamSet`给出了`Trafo is set.`的信息，  
  也可以通过`ParamSet$trafo()`手动输入信息来查看是否有转换
- 看看`ParamSet$trafo()`的转换
  - 结果也是完全一致

```r
svm$param_set$search_space()$trafo(list(cost = 1))
ss$trafo(list(cost = 1))
```

```R
$cost = 2.71828182845905
$cost = 2.71828182845905
```

- 从上面的例子可以看出，`to_tune(1e-5,1e5, logscale = TRUE)`的原理是：
  - 将下限与上限做对数变换
  - 在对数变换后的上下限形成的区间做均匀取样
  - 将均匀取样的结果进行指数变换，传递给`Learner`

- `to_tune()`除了接受`to_tune(1e-5, 1e5, logscale = TRUE)`这样的格式外，  
  也接受`Param`对象
- 在下面例子中，取样在区间$[0,3]$均匀取样，将取样的结果$+2$传递到`Learner`中

```r
lrn(
 "classif.svm",
 cost = to_tune(
  p_dbl(
   0, 3, 
   trafo = function(x) return(x + 2)
  )
 )
)
```

- 对于多个复杂变换，需要将函数传递到`.extra_trafo`中
  - `.extra_trafo`接受一个带有参数`x`和`param_set`的函数
  - 在调优过程中，  
   `x`是一个包含正在测试的配置的列表，  
   `param_set`是整个参数集
    - 测试了一下，在这个例子里省略`param_set`也是可以的，  
   但是不确定其他用法中能不能省，我觉得还是别省了

```r
search_space <- ps(
 # `cost`在区间[-1,1]均匀取样，取样结果做指数转换
 cost = p_dbl(-1, 1, trafo = function(x) return(exp(x))),
 # `kernel`有'polynomial'和'radial'两种选择
 kernel = p_fct(c('polynomial', 'radial')),
 # 传递一个函数：当kernel == 'polynomial'时，
 # `cost`抽样后指数转换的最终结果+2；否则就保持最终结果
 .extra_trafo = function(x, param_set) {
     if (x$kernel == 'polynomial') {
       x$cost = x$cost + 2
  }
  return(x)
 }
)

search_space$trafo(list(cost = 1, kernel = 'polynomial'))
search_space$trafo(list(cost = 1, kernel = 'radial'))
```

```css
$cost
4.71828182845904
$kernel
'polynomial'


$cost
2.71828182845905
$kernel
'radial'
```
